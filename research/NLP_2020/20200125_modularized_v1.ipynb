{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#import \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import bs4 as bs\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize # tokenizes sentences\n",
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "#Machine Learning\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import metrics # for confusion matrix, accuracy score etc\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from data_prep import DataPreprocessing as DP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = DP.data_preprocessing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_unit_state</th>\n",
       "      <th>choose_one_category</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>cat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>finalized</td>\n",
       "      <td>other_useful_information</td>\n",
       "      <td>Cyclone Pam hits New Zealand - dominion-post |...</td>\n",
       "      <td>storm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>finalized</td>\n",
       "      <td>infrastructure_and_utilities_damage</td>\n",
       "      <td>#CylonePam devastates #Vanuatu: Regional disas...</td>\n",
       "      <td>storm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>finalized</td>\n",
       "      <td>injured_or_dead_people</td>\n",
       "      <td>RT @9NewsAUS: Emergency response teams from Au...</td>\n",
       "      <td>storm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>finalized</td>\n",
       "      <td>other_useful_information</td>\n",
       "      <td>Vanuatu â°Â_ #RT Cyclone Pam Lashes Vanuatu ...</td>\n",
       "      <td>storm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>finalized</td>\n",
       "      <td>other_useful_information</td>\n",
       "      <td>Aid agencies: Vanuatu conditions more challeng...</td>\n",
       "      <td>storm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2008</td>\n",
       "      <td>golden</td>\n",
       "      <td>other_useful_information</td>\n",
       "      <td>An earthquake creates new island in #Pakistan!...</td>\n",
       "      <td>eq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2009</td>\n",
       "      <td>golden</td>\n",
       "      <td>not_related_or_irrelevant</td>\n",
       "      <td>RT @sairabaig: Rt@oOol_JaLoOl: Tabaaahi dar Ta...</td>\n",
       "      <td>eq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2010</td>\n",
       "      <td>golden</td>\n",
       "      <td>other_useful_information</td>\n",
       "      <td>RT @FoxNews: New massive earthquake rocks sout...</td>\n",
       "      <td>eq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2011</td>\n",
       "      <td>golden</td>\n",
       "      <td>injured_or_dead_people</td>\n",
       "      <td>20 more bodies recovered from a seminary in Ba...</td>\n",
       "      <td>eq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2012</td>\n",
       "      <td>golden</td>\n",
       "      <td>donation_needs_or_offers_or_volunteering_services</td>\n",
       "      <td>RT @AccessAid: After deadly #earthquake in #Pa...</td>\n",
       "      <td>eq</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23145 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     _unit_state                                choose_one_category  \\\n",
       "0      finalized                           other_useful_information   \n",
       "1      finalized                infrastructure_and_utilities_damage   \n",
       "2      finalized                             injured_or_dead_people   \n",
       "3      finalized                           other_useful_information   \n",
       "4      finalized                           other_useful_information   \n",
       "...          ...                                                ...   \n",
       "2008      golden                           other_useful_information   \n",
       "2009      golden                          not_related_or_irrelevant   \n",
       "2010      golden                           other_useful_information   \n",
       "2011      golden                             injured_or_dead_people   \n",
       "2012      golden  donation_needs_or_offers_or_volunteering_services   \n",
       "\n",
       "                                             tweet_text    cat  \n",
       "0     Cyclone Pam hits New Zealand - dominion-post |...  storm  \n",
       "1     #CylonePam devastates #Vanuatu: Regional disas...  storm  \n",
       "2     RT @9NewsAUS: Emergency response teams from Au...  storm  \n",
       "3     Vanuatu â°Â_ #RT Cyclone Pam Lashes Vanuatu ...  storm  \n",
       "4     Aid agencies: Vanuatu conditions more challeng...  storm  \n",
       "...                                                 ...    ...  \n",
       "2008  An earthquake creates new island in #Pakistan!...     eq  \n",
       "2009  RT @sairabaig: Rt@oOol_JaLoOl: Tabaaahi dar Ta...     eq  \n",
       "2010  RT @FoxNews: New massive earthquake rocks sout...     eq  \n",
       "2011  20 more bodies recovered from a seminary in Ba...     eq  \n",
       "2012  RT @AccessAid: After deadly #earthquake in #Pa...     eq  \n",
       "\n",
       "[23145 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "#from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['tweet_text']\n",
    "y = df['choose_one_category'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split( X, y , test_size = 0.2 , random_state = 42 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "764     Dude. Disease happens. You have #ObamaDerangem...\n",
       "979     10 Questions on the Deadly Middle Eastern Viru...\n",
       "286     RT @globalnews: Government says at least 29 Ca...\n",
       "342     Chile Earthquake Offers Reminder That U.S. Fac...\n",
       "1624    Army evacuates 20,000 people affected by flood...\n",
       "                              ...                        \n",
       "891     Nepal and India begin relief efforts as monsoo...\n",
       "443     Ã£â¬Â#USGS #alertÃ£â¬â M 2.0, Central Ala...\n",
       "359     Philippines residents flee as Typhoon Hagupit ...\n",
       "860     Lola is on cyclone watch! #bringthatrain! http...\n",
       "706     FOLLOW @iat_music EBOLA VIRUS FACTS AND FALLAC...\n",
       "Name: tweet_text, Length: 18516, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CT import OwnTransformer as OT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ot = OT()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pipeline = Pipeline(steps=[('text_cleaner', ot)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_clean = test_pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "764     Dude. Disease happens. You have #ObamaDerangem...\n",
       "979     10 Questions on the Deadly Middle Eastern Viru...\n",
       "286     RT @globalnews: Government says at least 29 Ca...\n",
       "342     Chile Earthquake Offers Reminder That U.S. Fac...\n",
       "1624    Army evacuates 20,000 people affected by flood...\n",
       "                              ...                        \n",
       "891     Nepal and India begin relief efforts as monsoo...\n",
       "443     Ã£â¬Â#USGS #alertÃ£â¬â M 2.0, Central Ala...\n",
       "359     Philippines residents flee as Typhoon Hagupit ...\n",
       "860     Lola is on cyclone watch! #bringthatrain! http...\n",
       "706     FOLLOW @iat_music EBOLA VIRUS FACTS AND FALLAC...\n",
       "Name: tweet_text, Length: 18516, dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_clean method default settings hashtag_just_sign=True, remove_stopwords=True, test_position=4\n",
    "df_clean = tweet_cleaner(df, remove_stopwords=True, hashtag_just_sign = False, tp_start=100, tp_end=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train on content: 'other_useful_information', 'infrastructure_and_utilities_damage',\n",
    "       'injured_or_dead_people', 'not_related_or_irrelevant',\n",
    "       'donation_needs_or_offers_or_volunteering_services',\n",
    "       'caution_and_advice', 'sympathy_and_emotional_support',\n",
    "       'missing_trapped_or_found_people',\n",
    "       'displaced_people_and_evacuations', 'affected_people',\n",
    "       'disease_signs_or_symptoms', 'prevention', 'disease_transmission',\n",
    "       'treatment', 'deaths_reports'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_content, output_content_importances, output_content_feature_names = training_on_content(df_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train on cat: 'storm', 'eq', 'flood', 'virus'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output_category, output_cat_importances, output_cat_feature_names = training_on_cat(df_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output the lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output_content)\n",
    "print(output_cat_importances)\n",
    "#control output that we put out the identical list in both cases\n",
    "print(output_cat_feature_names[1][100:110])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output)\n",
    "print(output_content_importances)\n",
    "#control output that we put out the identical list in both cases\n",
    "print(output_content_feature_names[1][100:110])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Twitter datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "source data from https://crisisnlp.qcri.org/lrec2016/lrec2016.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data presented by:\n",
    "- Muhammad Imran, Prasenjit Mitra, Carlos Castillo: Twitter as a Lifeline: Human-annotated Twitter Corpora for NLP of Crisis-related Messages. In Proceedings of the 10th Language Resources and Evaluation Conference (LREC), pp. 1638-1643. May 2016, Portorož, Slovenia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cleaning steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_cleaner (dfc, hashtag_just_sign=True, remove_stopwords=True, tp_start=4, tp_end=10, cleaner='self_made'):  \n",
    "    list_before = list()\n",
    "    list_after = list()\n",
    "    \n",
    "    #check one example before cleaning\n",
    "    print('example before cleaning at positions ', tp_start, ' to ', tp_end)\n",
    "    #for row in (dfc.iloc[tp_start:tp_end]).rows:\n",
    "    #   print(row['tweet_text'], '  ---  ',row['choose_one_category'])\n",
    "\n",
    "    for index, row in dfc.iloc[tp_start:tp_end].iterrows():\n",
    "        list_before.append([row['tweet_text'], row['choose_one_category']])\n",
    "        \n",
    "    if(cleaner=='off_the_shelf'):\n",
    "        print('Not defined pre-processor, can\\'t be used')\n",
    "    \n",
    "    if(cleaner=='self_made'):\n",
    "        #IN ANY CASE \n",
    "        #non alpabetical/ numerical\n",
    "        dfc = dfc.replace(to_replace =r'&amp;', value = '', regex = True)\n",
    "        dfc = dfc.replace(to_replace =r'&gt;', value = '', regex = True)\n",
    "        #hyperlinks\n",
    "        dfc = dfc.replace(to_replace =r'http\\S+', value = '', regex = True)\n",
    "        #usernames\n",
    "        dfc = dfc.replace(to_replace =r'@\\S+', value = '', regex = True) \n",
    "        #remove retweet\n",
    "        dfc = dfc.replace(to_replace ='RT :', value = '', regex = True) \n",
    "        dfc = dfc.replace(to_replace ='RT ', value = '', regex = True)\n",
    "\n",
    "\n",
    "        #HASHTAG OPTIONAL REMOVE\n",
    "        if(hashtag_just_sign == True):\n",
    "            dfc = dfc.replace(to_replace ='#', value = '', regex = True) \n",
    "        else:\n",
    "            dfc = dfc.replace(to_replace =r'#\\S+', value = '', regex = True)\n",
    "\n",
    "        #IN ANY CASE\n",
    "        #remove punctation\n",
    "        dfc = dfc.replace(to_replace ='[\",:!?\\\\-]', value = ' ', regex = True)\n",
    "        #4. Tokenize into words (all lower case)\n",
    "        dfc.tweet_text = dfc.tweet_text.str.lower()\n",
    "        #make sure no weird letters left\n",
    "\n",
    "        #u = df.select_dtypes(object)\n",
    "        dfc['tweet_text'] = dfc['tweet_text'].apply(lambda x: x.encode('ascii', 'ignore').decode('ascii'))\n",
    "\n",
    "        if(remove_stopwords == True):\n",
    "            dfc.tweet_text = dfc.tweet_text.str.split() \n",
    "            eng_stopwords = set(stopwords.words(\"english\"))\n",
    "            dfc['tweet_text'] = dfc['tweet_text'].apply(lambda x: [item for item in x if item not in eng_stopwords])\n",
    "            #join the list items back to one string\n",
    "            dfc['tweet_text'] = dfc['tweet_text'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "    \n",
    "    \n",
    "    #print('example after cleaning at positions ', tp_start, ' to ', tp_end)\n",
    "    #print(dfc.iloc[tp_start:tp_end].tweet_text)\n",
    "    \n",
    "    for index, row in dfc.iloc[tp_start:tp_end].iterrows():\n",
    "        list_after.append([row['tweet_text'], row['choose_one_category']])\n",
    "    \n",
    "    \n",
    "    for num in range(0, len(list_before)):\n",
    "        print(list_before[num])\n",
    "        print(list_after[num])\n",
    "        print('--------------------------------------------------------------')\n",
    "    \n",
    "    return dfc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file=df_clean.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_on_content(file):\n",
    "    content = list(file.choose_one_category.unique())\n",
    "    content_importances = list()\n",
    "    content_feature_names = list()\n",
    "\n",
    "    for con in content:\n",
    "        #build the needed dataset\n",
    "        temp_file = file.copy(deep=True)\n",
    "        replacing_list = list()\n",
    "        replacing_list = content.copy()\n",
    "        replacing_list.remove(con)\n",
    "        #print(replacing_list)\n",
    "        string_to_be_added = str('no_'+ con)\n",
    "        #print(string_to_be_added)\n",
    "        print('testing ...')\n",
    "        #print(replacing_list)\n",
    "        temp_file.choose_one_category = temp_file.choose_one_category.replace(to_replace=replacing_list, value=string_to_be_added) \n",
    "        print('                         ')\n",
    "        print('Train forest for ', con)\n",
    "        print('---------------------------')\n",
    "        forest_out, vectorizer_out, feature_names_out, importances_out = train_forest(temp_file['tweet_text'], temp_file['choose_one_category'])\n",
    "        content_feature_names.append(feature_names_out)\n",
    "        content_importances.append(importances_out)\n",
    "\n",
    "    print('DONE with Training on Content!!!')\n",
    "    \n",
    "    return conent, content_importances, content_feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_on_cat(file):    \n",
    "    category = list(file.cat.unique())\n",
    "    cat_importances = list()\n",
    "    cat_feature_names = list()\n",
    "\n",
    "    for cat in category:\n",
    "        #build the needed dataset\n",
    "        temp_file = file.copy(deep=True)\n",
    "        replacing_list = list()\n",
    "        replacing_list = category.copy()\n",
    "        replacing_list.remove(cat)\n",
    "        #print(replacing_list)\n",
    "        string_to_be_added = str('no_'+ cat)\n",
    "        #print(string_to_be_added)\n",
    "        print('testing ...')\n",
    "        #print(replacing_list)\n",
    "        temp_file.cat = temp_file.cat.replace(to_replace=replacing_list, value=string_to_be_added) \n",
    "        print('                         ')\n",
    "        print('Train forest for ', cat)\n",
    "        print('---------------------------')\n",
    "        forest_out, vectorizer_out, feature_names_out, importances_out = train_forest(temp_file['tweet_text'], temp_file['cat'])\n",
    "        cat_feature_names.append(feature_names_out)\n",
    "        cat_importances.append(importances_out)\n",
    "\n",
    "    print('DONE with Training on Cat!!!')\n",
    "    \n",
    "    return content, cat_importances, cat_feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Training Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_forest(data, y):\n",
    "\n",
    "    print(\"Creating the bag of words model!\\n\")\n",
    "    # CountVectorizer\" is scikit-learn's bag of words tool, here we show more keywords \n",
    "    vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                                 tokenizer = None,    \\\n",
    "                                 preprocessor = None, \\\n",
    "                                 stop_words = None,   \\\n",
    "                                 max_features = 2000) \n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\\\n",
    "    data, y, random_state=0, test_size=.2)\n",
    "\n",
    "    # Then we use fit_transform() to fit the model / learn the vocabulary,\n",
    "    # then transform the data into feature vectors.\n",
    "    # The input should be a list of strings. .toarraty() converts to a numpy array\n",
    "    \n",
    "    train_bag = vectorizer.fit_transform(X_train).toarray()\n",
    "    test_bag = vectorizer.transform(X_test).toarray()\n",
    "\n",
    "    # You can extract the vocabulary created by CountVectorizer\n",
    "    # by running print(vectorizer.get_feature_names())\n",
    "\n",
    "\n",
    "    print(\"Training the random forest classifier!\\n\")\n",
    "    # Initialize a Random Forest classifier with 75 trees\n",
    "    forest = RandomForestClassifier(n_estimators = 50) \n",
    "\n",
    "    # Fit the forest to the training set, using the bag of words as \n",
    "    # features and the sentiment labels as the target variable\n",
    "    forest = forest.fit(train_bag, y_train)\n",
    "\n",
    "    train_predictions = forest.predict(train_bag)\n",
    "    test_predictions = forest.predict(test_bag)\n",
    "    \n",
    "    train_acc = metrics.accuracy_score(y_train, train_predictions)\n",
    "    valid_acc = metrics.accuracy_score(y_test, test_predictions)\n",
    "    print(\"The training accuracy is: \", train_acc, \"\\n\", \"The validation accuracy is: \", valid_acc)\n",
    "    \n",
    "    print('confusion matrix')\n",
    "    print(metrics.confusion_matrix(y_test,test_predictions))\n",
    "    \n",
    "    print('check classes on which we trained')\n",
    "    print(forest.classes_)\n",
    "    \n",
    "    importances = forest.feature_importances_\n",
    "    # returns relative importance of all features.\n",
    "    # they are in the order of the columns\n",
    "    print(importances)\n",
    "    len(importances)\n",
    "\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    # sort importance scores\n",
    "    \n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    # Print the feature ranking\n",
    "    print(\"Feature ranking:\")\n",
    "    top_50 = indices[:50]\n",
    "    top50_features = [vectorizer.get_feature_names()[ind] for ind in top_50]\n",
    "    print(top50_features)\n",
    "    \n",
    "    return(forest, vectorizer, feature_names, importances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
