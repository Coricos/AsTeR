{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Readings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Text summarizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on https://towardsdatascience.com/summarizing-tweets-in-a-disaster-part-ii-67db021d378d:\n",
    "- look for situational words, describing situation or casulties using SpaCy (Numerals (eg. number of casualties, important phone numbers); Entities (eg. places, dates, events, organisations, etc.))\n",
    "    - use entity-types, look for content words\n",
    "- tf-idf score (rank somthing like \"Nepal\" highly, but not \"the\") --> use Textacy\n",
    "- clean data before tokenizing: abbreviations, misspellings (NLTK has a twitter-specific tokenizer)\n",
    "- summary of words as an ILP problem\n",
    "\n",
    "check also the notebooks\n",
    "- for SpaCy: https://github.com/gabrieltseng/datascience-projects/blob/master/natural_language_processing/twitter_disasters/spaCy/3%20-%20Abstractive%20Summary.ipynb\n",
    "- for NLTK: https://github.com/gabrieltseng/datascience-projects/blob/master/natural_language_processing/twitter_disasters/NLTK/3%20-%20Abstractive%20Summary.ipynb\n",
    "\n",
    "IBM Watson research paper\n",
    "- https://arxiv.org/pdf/1602.06023.pdf\n",
    "\n",
    "Tensorflow text summarization model\n",
    "- https://github.com/tensorflow/models/tree/master/research/textsum\n",
    "\n",
    "API services\n",
    "- https://smmry.com/api\n",
    "\n",
    "Facebook AI research: A Neural Attention Model for Abstractive Sentence Summarization\n",
    "- https://arxiv.org/pdf/1509.00685.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ideas for overall approach: use occuring tweets as well (e.g. twitter set for wildfire)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Keyword Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- based on https://medium.com/analytics-vidhya/automated-keyword-extraction-from-articles-using-nlp-bfd864f41b34\n",
    "- also very interesting points on text pre-processing in here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"KeyWordExtraction_HighLevel.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Futher NLP Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- word embeddings: https://www.wikiwand.com/en/Word_embedding --> check word2vec\n",
    "- sentiment analysis: https://www.wikiwand.com/en/Sentiment_analysis\n",
    "    - for background on singular value decomposition https://www.wikiwand.com/en/Singular_value_decomposition\n",
    "- part-of-speech (POS) tagging\n",
    "- using word graphs (powerful when there are multiple sentences describing similar situations)\n",
    "- linguistic quality: compare my sample sentence to \"normal\" English sentences\n",
    "    - see also KenLM tool at https://kheafield.com/code/kenlm/\n",
    "    - and more readings to understand this challenge http://masatohagiwara.net/training-an-n-gram-language-model-and-estimating-sentence-probability.html\n",
    "    - can be compared to current \"correct\" American English https://www.english-corpora.org/coca/\n",
    "- spell checker: https://pypi.org/project/pyspellchecker/\n",
    "- regular expressions: https://docs.python.org/3/library/re.html\n",
    "- term frequency * Inverse Document Frequency: https://hackernoon.com/finding-the-most-important-sentences-using-nlp-tf-idf-3065028897a3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pre-trained language models\n",
    "- ELMo: https://arxiv.org/abs/1802.05365\n",
    "- ULMFiT: https://arxiv.org/abs/1801.06146\n",
    "- OpenAI Transformer: https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf\n",
    "- BERT: https://arxiv.org/abs/1810.04805"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP trends\n",
    "- Commonsense Interference like Event2Mind (https://arxiv.org/pdf/1805.06939.pdf) or SWAG (https://arxiv.org/abs/1808.05326)\n",
    "\n",
    "- summary of trends to be found here: http://ruder.io/10-exciting-ideas-of-2018-in-nlp/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### more research to be done into\n",
    "- general summarization\n",
    "- statistical parsing\n",
    "- knowledge extraction: are 911 calls given in a standard or re-occuring format?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of current trends in NLP\n",
    "- https://www.analyticsvidhya.com/blog/2017/10/essential-nlp-guide-data-scientists-top-10-nlp-tasks/ (includes a lot of interesting and helpful links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. DL/ ML tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- transfer learning: https://machinelearningmastery.com/transfer-learning-for-deep-learning/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Disaster datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Twitter datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://arxiv.org/abs/1605.05894\n",
    "- https://www.aaai.org/ocs/index.php/ICWSM/ICWSM11/paper/view/2834\n",
    "- https://dl.acm.org/citation.cfm?id=2914600"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Other datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://data.world/crowdflower/disasters-on-social-media\n",
    "- collection of different datasets: https://crisisnlp.qcri.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Other github links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Twitter: disaster classification, sentiment analysis, named entity recognition --> https://github.com/glrn/nlp-disaster-analysis\n",
    "- Natural Language Understanding Bot translating unstructured text into structured data --> https://github.com/Kontikilabs/alter-nlu\n",
    "- Emogram (Text Analysis for unstructured text): Acronym Resolution, Auto Corect, Key Phrase Extraction, Polarity Detection --> https://github.com/axenhammer/Emogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import bs4 as bs\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize # tokenizes sentences\n",
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "eng_stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transcript input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "call1_time = \"9:35:39\"\n",
    "call1_length = 24 #length in seconds\n",
    "\n",
    "call1_text = [\"OPERATOR: Newtown 911. What's the location of your emergency?\", \n",
    "              \"CALLER: Hi, Sandy Hook School. I think there is somebody shooting in here, in Sandy Hook School.\",\n",
    "              \"OPERATOR: O.K. What makes you think that?\",\n",
    "              \"CALLER: Because somebody's got a gun. I caught a glimpse of someone, they're running down the hallway.\",\n",
    "              \"OPERATOR: Okay.\",\n",
    "              \"CALLER: They are still running. They're still shooting.\",\n",
    "              \"CALLER: Sandy Hook School, please.\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing the emergency call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " hi sandy hook school i think there is somebody shooting in here in sandy hook school because somebodys got a gun i caught a glimpse of someone theyre running down the hallway they are still running theyre still shooting sandy hook school please\n",
      "['hi', 'sandy', 'hook', 'school', 'i', 'think', 'there', 'is', 'somebody', 'shooting', 'in', 'here', 'in', 'sandy', 'hook', 'school', 'because', 'somebodys', 'got', 'a', 'gun', 'i', 'caught', 'a', 'glimpse', 'of', 'someone', 'theyre', 'running', 'down', 'the', 'hallway', 'they', 'are', 'still', 'running', 'theyre', 'still', 'shooting', 'sandy', 'hook', 'school', 'please']\n"
     ]
    }
   ],
   "source": [
    "operator = []\n",
    "caller = []\n",
    "\n",
    "for statement in call1_text:\n",
    "    if 'OPERATOR' in statement:\n",
    "        statement = statement.replace(\"OPERATOR:\", \"\")\n",
    "        operator.append(statement)\n",
    "    if 'CALLER' in statement:\n",
    "        statement = statement.replace(\"CALLER:\", \"\")\n",
    "        caller.append(statement)\n",
    "\n",
    "tsc = \"\"\n",
    "for part in caller:\n",
    "    #remove punctation and special characters\n",
    "    part = re.sub('[^a-zA-Z ]' ,'',part)\n",
    "    #all to lower case\n",
    "    part = part.lower()\n",
    "    #part_words = part.split()\n",
    "    tsc += part\n",
    "       \n",
    "print(tsc)\n",
    "tsc_words = tsc.split()\n",
    "print(tsc_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correct grammer\n",
    "correct grammer, something along these lines: https://pypi.org/project/pyspellchecker/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named entity recognition/ disambiguiation\n",
    "- find out name of school, city, street etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis\n",
    "- - sentiment analysis \n",
    "    - check paper at https://www.analyticsvidhya.com/blog/2017/01/sentiment-analysis-of-twitter-posts-on-chennai-floods-using-python/, where sentiment analysis was performed on Chennai flood dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part of Speech Tagging\n",
    "For meaning of tags check: https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "\n",
    "implement more advanced algorithm that can also identify locations etc. and not only single words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('hi', 'NN'), ('sandy', 'NN'), ('hook', 'NN'), ('school', 'NN'), ('i', 'NN'), ('think', 'VBP'), ('there', 'EX'), ('is', 'VBZ'), ('somebody', 'NN'), ('shooting', 'VBG'), ('in', 'IN'), ('here', 'RB'), ('in', 'IN'), ('sandy', 'JJ'), ('hook', 'NN'), ('school', 'NN'), ('because', 'IN'), ('somebodys', 'NN'), ('got', 'VBD'), ('a', 'DT'), ('gun', 'NN'), ('i', 'NN'), ('caught', 'VBD'), ('a', 'DT'), ('glimpse', 'NN'), ('of', 'IN'), ('someone', 'NN'), ('theyre', 'NN'), ('running', 'VBG'), ('down', 'RP'), ('the', 'DT'), ('hallway', 'NN'), ('they', 'PRP'), ('are', 'VBP'), ('still', 'RB'), ('running', 'VBG'), ('theyre', 'NN'), ('still', 'RB'), ('shooting', 'VBG'), ('sandy', 'JJ'), ('hook', 'NN'), ('school', 'NN'), ('please', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "#part of speech tagging - standard NLTK probably not sufficient\n",
    "token_tag = pos_tag(tsc_words)\n",
    "print(token_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output: counting expressions (like Sandy Hook School or shooting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return 'n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi sandy hook school i think there be somebody shoot in here in sandy hook school because somebody get a gun i catch a glimpse of someone theyre run down the hallway they be still run theyre still shoot sandy hook school please\n"
     ]
    }
   ],
   "source": [
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "wnl_stems = []\n",
    "for pair in token_tag:\n",
    "    res = wnl.lemmatize(pair[0],pos=get_wordnet_pos(pair[1]))\n",
    "    wnl_stems.append(res)\n",
    "\n",
    "print(' '.join(wnl_stems))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REVIEW WITHOUT STOPWORDS:\n",
      "hi sandy hook school think somebody shooting sandy hook school somebodys got gun caught glimpse someone theyre running hallway still running theyre still shooting sandy hook school please\n",
      "\n",
      "Stop words removed ['i', 'there', 'is', 'in', 'here', 'in', 'because', 'a', 'i', 'a', 'of', 'down', 'the', 'they', 'are']\n",
      "\n",
      "NUMBER OF STOPWORDS REMOVED: 15\n"
     ]
    }
   ],
   "source": [
    "tsc_wo_stopwords = [w for w in tsc_words if not w in stopwords.words(\"english\")]\n",
    "removed_stopwords = [w for w in tsc_words if w in stopwords.words(\"english\")]\n",
    "\n",
    "print('REVIEW WITHOUT STOPWORDS:')\n",
    "print(' '.join(tsc_wo_stopwords))\n",
    "print()\n",
    "print('Stop words removed', removed_stopwords)\n",
    "print()\n",
    "print('NUMBER OF STOPWORDS REMOVED:',len(removed_stopwords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### here follows a summary of what we extracted from the text (summary, keywords etc.) and how this influences the priority"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
