{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import bs4 as bs\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize # tokenizes sentences\n",
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "eng_stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load more data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "source data from https://crisisnlp.qcri.org/lrec2016/lrec2016.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eq - earthquake\n",
    "\n",
    "eq_pakistan_2013 = pd.read_csv('data/2013_pakistan_eq.csv', skip_blank_lines=True, encoding = \"ISO-8859-1\")\n",
    "eq_pakistan_2013['cat'] = 'eq_pakistan_2013'\n",
    "\n",
    "eq_california_2014 = pd.read_csv('data/2014_california_eq.csv', skip_blank_lines=True, encoding = \"ISO-8859-1\")\n",
    "eq_california_2014['cat'] = 'eq_california_2014'\n",
    "\n",
    "eq_chile_2014 = pd.read_csv('data/2014_chile_eq_en.csv', skip_blank_lines=True, encoding = \"ISO-8859-1\")\n",
    "eq_chile_2014['cat'] = 'eq_chile_2014'\n",
    "\n",
    "ebola_virus_2014 = pd.read_csv('data/2014_ebola_virus.csv', skip_blank_lines=True, encoding = \"ISO-8859-1\")\n",
    "ebola_virus_2014['cat'] = 'ebola_virus_2014'\n",
    "\n",
    "hurricane_odile_2014 = pd.read_csv('data/2014_hurricane_odile.csv', skip_blank_lines=True, encoding = \"ISO-8859-1\")\n",
    "hurricane_odile_2014['cat'] = 'hurricane_odile_2014'\n",
    "\n",
    "flood_india_2014 = pd.read_csv('data/2014_india_floods.csv', skip_blank_lines=True, encoding = \"ISO-8859-1\")\n",
    "flood_india_2014['cat'] = 'flood_india_2014'\n",
    "\n",
    "middle_east_respiratory_2014 = pd.read_csv('data/2014_mers_cf_labels.csv', skip_blank_lines=True, encoding = \"ISO-8859-1\")\n",
    "middle_east_respiratory_2014['cat'] = 'middle_east_respiratory_2014'\n",
    "\n",
    "flood_pakistan_2014 = pd.read_csv('data/2014_pakistan_floods_cf_labels.csv', skip_blank_lines=True, encoding = \"ISO-8859-1\")\n",
    "flood_pakistan_2014['cat'] = 'flood_pakistan_2014'\n",
    "\n",
    "typhoon_philippines_2014 = pd.read_csv('data/2014_typhoon_hagupit_cf_labels.csv', skip_blank_lines=True, encoding = \"ISO-8859-1\")\n",
    "typhoon_philippines_2014['cat']='typhoon_philippines_2014'\n",
    "\n",
    "cyclone_pam_2015 = pd.read_csv('data/2015_cyclone_pam_cf_labels.csv', skip_blank_lines=True, encoding = \"ISO-8859-1\")\n",
    "cyclone_pam_2015['cat'] = 'cyclone_pam_2015'\n",
    "\n",
    "eq_nepal_2015 = pd.read_csv('data/2015_nepal_eq_cf_labels.csv', skip_blank_lines=True, encoding = \"ISO-8859-1\")\n",
    "eq_nepal_2015['cat'] = 'eq_nepal_2015'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = pd.concat([cyclone_pam_2015, eq_nepal_2015, typhoon_philippines_2014, flood_pakistan_2014,\n",
    "                middle_east_respiratory_2014, flood_india_2014, hurricane_odile_2014, \n",
    "                ebola_virus_2014, eq_chile_2014, eq_california_2014, eq_pakistan_2013])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23146"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_unit_id</th>\n",
       "      <th>_golden</th>\n",
       "      <th>_unit_state</th>\n",
       "      <th>_trusted_judgments</th>\n",
       "      <th>_last_judgment_at</th>\n",
       "      <th>choose_one_category:confidence</th>\n",
       "      <th>choose_one_category_gold</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>cat</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>choose_one_category</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>affected_people</th>\n",
       "      <td>866</td>\n",
       "      <td>866</td>\n",
       "      <td>866</td>\n",
       "      <td>866</td>\n",
       "      <td>860</td>\n",
       "      <td>866</td>\n",
       "      <td>6</td>\n",
       "      <td>866</td>\n",
       "      <td>866</td>\n",
       "      <td>866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>caution_and_advice</th>\n",
       "      <td>1063</td>\n",
       "      <td>1063</td>\n",
       "      <td>1063</td>\n",
       "      <td>1063</td>\n",
       "      <td>1053</td>\n",
       "      <td>1063</td>\n",
       "      <td>10</td>\n",
       "      <td>1063</td>\n",
       "      <td>1063</td>\n",
       "      <td>1063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>deaths_reports</th>\n",
       "      <td>94</td>\n",
       "      <td>94</td>\n",
       "      <td>94</td>\n",
       "      <td>94</td>\n",
       "      <td>93</td>\n",
       "      <td>94</td>\n",
       "      <td>1</td>\n",
       "      <td>93</td>\n",
       "      <td>93</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disease_signs_or_symptoms</th>\n",
       "      <td>431</td>\n",
       "      <td>431</td>\n",
       "      <td>431</td>\n",
       "      <td>431</td>\n",
       "      <td>425</td>\n",
       "      <td>431</td>\n",
       "      <td>8</td>\n",
       "      <td>431</td>\n",
       "      <td>431</td>\n",
       "      <td>431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disease_transmission</th>\n",
       "      <td>425</td>\n",
       "      <td>425</td>\n",
       "      <td>425</td>\n",
       "      <td>425</td>\n",
       "      <td>420</td>\n",
       "      <td>425</td>\n",
       "      <td>6</td>\n",
       "      <td>425</td>\n",
       "      <td>425</td>\n",
       "      <td>425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>displaced_people_and_evacuations</th>\n",
       "      <td>633</td>\n",
       "      <td>633</td>\n",
       "      <td>633</td>\n",
       "      <td>633</td>\n",
       "      <td>625</td>\n",
       "      <td>633</td>\n",
       "      <td>8</td>\n",
       "      <td>633</td>\n",
       "      <td>633</td>\n",
       "      <td>633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>donation_needs_or_offers_or_volunteering_services</th>\n",
       "      <td>2610</td>\n",
       "      <td>2610</td>\n",
       "      <td>2610</td>\n",
       "      <td>2610</td>\n",
       "      <td>2591</td>\n",
       "      <td>2610</td>\n",
       "      <td>19</td>\n",
       "      <td>2610</td>\n",
       "      <td>2610</td>\n",
       "      <td>2610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>infrastructure_and_utilities_damage</th>\n",
       "      <td>1851</td>\n",
       "      <td>1851</td>\n",
       "      <td>1851</td>\n",
       "      <td>1851</td>\n",
       "      <td>1835</td>\n",
       "      <td>1851</td>\n",
       "      <td>16</td>\n",
       "      <td>1851</td>\n",
       "      <td>1851</td>\n",
       "      <td>1851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>injured_or_dead_people</th>\n",
       "      <td>2510</td>\n",
       "      <td>2510</td>\n",
       "      <td>2510</td>\n",
       "      <td>2510</td>\n",
       "      <td>2490</td>\n",
       "      <td>2510</td>\n",
       "      <td>19</td>\n",
       "      <td>2510</td>\n",
       "      <td>2510</td>\n",
       "      <td>2510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>missing_trapped_or_found_people</th>\n",
       "      <td>402</td>\n",
       "      <td>402</td>\n",
       "      <td>402</td>\n",
       "      <td>402</td>\n",
       "      <td>397</td>\n",
       "      <td>402</td>\n",
       "      <td>5</td>\n",
       "      <td>402</td>\n",
       "      <td>402</td>\n",
       "      <td>402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>not_related_or_irrelevant</th>\n",
       "      <td>2598</td>\n",
       "      <td>2598</td>\n",
       "      <td>2598</td>\n",
       "      <td>2598</td>\n",
       "      <td>2586</td>\n",
       "      <td>2598</td>\n",
       "      <td>12</td>\n",
       "      <td>2598</td>\n",
       "      <td>2598</td>\n",
       "      <td>2598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>other_useful_information</th>\n",
       "      <td>6984</td>\n",
       "      <td>6984</td>\n",
       "      <td>6984</td>\n",
       "      <td>6984</td>\n",
       "      <td>6959</td>\n",
       "      <td>6984</td>\n",
       "      <td>27</td>\n",
       "      <td>6984</td>\n",
       "      <td>6984</td>\n",
       "      <td>6984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prevention</th>\n",
       "      <td>288</td>\n",
       "      <td>288</td>\n",
       "      <td>288</td>\n",
       "      <td>288</td>\n",
       "      <td>286</td>\n",
       "      <td>288</td>\n",
       "      <td>2</td>\n",
       "      <td>288</td>\n",
       "      <td>288</td>\n",
       "      <td>288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sympathy_and_emotional_support</th>\n",
       "      <td>1969</td>\n",
       "      <td>1969</td>\n",
       "      <td>1969</td>\n",
       "      <td>1969</td>\n",
       "      <td>1956</td>\n",
       "      <td>1969</td>\n",
       "      <td>13</td>\n",
       "      <td>1969</td>\n",
       "      <td>1969</td>\n",
       "      <td>1969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>treatment</th>\n",
       "      <td>422</td>\n",
       "      <td>422</td>\n",
       "      <td>422</td>\n",
       "      <td>422</td>\n",
       "      <td>419</td>\n",
       "      <td>422</td>\n",
       "      <td>3</td>\n",
       "      <td>422</td>\n",
       "      <td>422</td>\n",
       "      <td>422</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   _unit_id  _golden  \\\n",
       "choose_one_category                                                    \n",
       "affected_people                                         866      866   \n",
       "caution_and_advice                                     1063     1063   \n",
       "deaths_reports                                           94       94   \n",
       "disease_signs_or_symptoms                               431      431   \n",
       "disease_transmission                                    425      425   \n",
       "displaced_people_and_evacuations                        633      633   \n",
       "donation_needs_or_offers_or_volunteering_services      2610     2610   \n",
       "infrastructure_and_utilities_damage                    1851     1851   \n",
       "injured_or_dead_people                                 2510     2510   \n",
       "missing_trapped_or_found_people                         402      402   \n",
       "not_related_or_irrelevant                              2598     2598   \n",
       "other_useful_information                               6984     6984   \n",
       "prevention                                              288      288   \n",
       "sympathy_and_emotional_support                         1969     1969   \n",
       "treatment                                               422      422   \n",
       "\n",
       "                                                   _unit_state  \\\n",
       "choose_one_category                                              \n",
       "affected_people                                            866   \n",
       "caution_and_advice                                        1063   \n",
       "deaths_reports                                              94   \n",
       "disease_signs_or_symptoms                                  431   \n",
       "disease_transmission                                       425   \n",
       "displaced_people_and_evacuations                           633   \n",
       "donation_needs_or_offers_or_volunteering_services         2610   \n",
       "infrastructure_and_utilities_damage                       1851   \n",
       "injured_or_dead_people                                    2510   \n",
       "missing_trapped_or_found_people                            402   \n",
       "not_related_or_irrelevant                                 2598   \n",
       "other_useful_information                                  6984   \n",
       "prevention                                                 288   \n",
       "sympathy_and_emotional_support                            1969   \n",
       "treatment                                                  422   \n",
       "\n",
       "                                                   _trusted_judgments  \\\n",
       "choose_one_category                                                     \n",
       "affected_people                                                   866   \n",
       "caution_and_advice                                               1063   \n",
       "deaths_reports                                                     94   \n",
       "disease_signs_or_symptoms                                         431   \n",
       "disease_transmission                                              425   \n",
       "displaced_people_and_evacuations                                  633   \n",
       "donation_needs_or_offers_or_volunteering_services                2610   \n",
       "infrastructure_and_utilities_damage                              1851   \n",
       "injured_or_dead_people                                           2510   \n",
       "missing_trapped_or_found_people                                   402   \n",
       "not_related_or_irrelevant                                        2598   \n",
       "other_useful_information                                         6984   \n",
       "prevention                                                        288   \n",
       "sympathy_and_emotional_support                                   1969   \n",
       "treatment                                                         422   \n",
       "\n",
       "                                                   _last_judgment_at  \\\n",
       "choose_one_category                                                    \n",
       "affected_people                                                  860   \n",
       "caution_and_advice                                              1053   \n",
       "deaths_reports                                                    93   \n",
       "disease_signs_or_symptoms                                        425   \n",
       "disease_transmission                                             420   \n",
       "displaced_people_and_evacuations                                 625   \n",
       "donation_needs_or_offers_or_volunteering_services               2591   \n",
       "infrastructure_and_utilities_damage                             1835   \n",
       "injured_or_dead_people                                          2490   \n",
       "missing_trapped_or_found_people                                  397   \n",
       "not_related_or_irrelevant                                       2586   \n",
       "other_useful_information                                        6959   \n",
       "prevention                                                       286   \n",
       "sympathy_and_emotional_support                                  1956   \n",
       "treatment                                                        419   \n",
       "\n",
       "                                                   choose_one_category:confidence  \\\n",
       "choose_one_category                                                                 \n",
       "affected_people                                                               866   \n",
       "caution_and_advice                                                           1063   \n",
       "deaths_reports                                                                 94   \n",
       "disease_signs_or_symptoms                                                     431   \n",
       "disease_transmission                                                          425   \n",
       "displaced_people_and_evacuations                                              633   \n",
       "donation_needs_or_offers_or_volunteering_services                            2610   \n",
       "infrastructure_and_utilities_damage                                          1851   \n",
       "injured_or_dead_people                                                       2510   \n",
       "missing_trapped_or_found_people                                               402   \n",
       "not_related_or_irrelevant                                                    2598   \n",
       "other_useful_information                                                     6984   \n",
       "prevention                                                                    288   \n",
       "sympathy_and_emotional_support                                               1969   \n",
       "treatment                                                                     422   \n",
       "\n",
       "                                                   choose_one_category_gold  \\\n",
       "choose_one_category                                                           \n",
       "affected_people                                                           6   \n",
       "caution_and_advice                                                       10   \n",
       "deaths_reports                                                            1   \n",
       "disease_signs_or_symptoms                                                 8   \n",
       "disease_transmission                                                      6   \n",
       "displaced_people_and_evacuations                                          8   \n",
       "donation_needs_or_offers_or_volunteering_services                        19   \n",
       "infrastructure_and_utilities_damage                                      16   \n",
       "injured_or_dead_people                                                   19   \n",
       "missing_trapped_or_found_people                                           5   \n",
       "not_related_or_irrelevant                                                12   \n",
       "other_useful_information                                                 27   \n",
       "prevention                                                                2   \n",
       "sympathy_and_emotional_support                                           13   \n",
       "treatment                                                                 3   \n",
       "\n",
       "                                                   tweet_id  tweet_text   cat  \n",
       "choose_one_category                                                            \n",
       "affected_people                                         866         866   866  \n",
       "caution_and_advice                                     1063        1063  1063  \n",
       "deaths_reports                                           93          93    94  \n",
       "disease_signs_or_symptoms                               431         431   431  \n",
       "disease_transmission                                    425         425   425  \n",
       "displaced_people_and_evacuations                        633         633   633  \n",
       "donation_needs_or_offers_or_volunteering_services      2610        2610  2610  \n",
       "infrastructure_and_utilities_damage                    1851        1851  1851  \n",
       "injured_or_dead_people                                 2510        2510  2510  \n",
       "missing_trapped_or_found_people                         402         402   402  \n",
       "not_related_or_irrelevant                              2598        2598  2598  \n",
       "other_useful_information                               6984        6984  6984  \n",
       "prevention                                              288         288   288  \n",
       "sympathy_and_emotional_support                         1969        1969  1969  \n",
       "treatment                                               422         422   422  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file.groupby(by='choose_one_category').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['_unit_id', '_golden', '_unit_state', '_trusted_judgments',\n",
       "       '_last_judgment_at', 'choose_one_category',\n",
       "       'choose_one_category:confidence', 'choose_one_category_gold',\n",
       "       'tweet_id', 'tweet_text', 'cat'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = file.drop(['_unit_id', '_golden', '_trusted_judgments',\n",
    "       '_last_judgment_at', 'choose_one_category:confidence', 'choose_one_category_gold',\n",
    "       'tweet_id'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = file.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_unit_state</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>cat</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>choose_one_category</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>affected_people</th>\n",
       "      <td>866</td>\n",
       "      <td>866</td>\n",
       "      <td>866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>caution_and_advice</th>\n",
       "      <td>1063</td>\n",
       "      <td>1063</td>\n",
       "      <td>1063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>deaths_reports</th>\n",
       "      <td>93</td>\n",
       "      <td>93</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disease_signs_or_symptoms</th>\n",
       "      <td>431</td>\n",
       "      <td>431</td>\n",
       "      <td>431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disease_transmission</th>\n",
       "      <td>425</td>\n",
       "      <td>425</td>\n",
       "      <td>425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>displaced_people_and_evacuations</th>\n",
       "      <td>633</td>\n",
       "      <td>633</td>\n",
       "      <td>633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>donation_needs_or_offers_or_volunteering_services</th>\n",
       "      <td>2610</td>\n",
       "      <td>2610</td>\n",
       "      <td>2610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>infrastructure_and_utilities_damage</th>\n",
       "      <td>1851</td>\n",
       "      <td>1851</td>\n",
       "      <td>1851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>injured_or_dead_people</th>\n",
       "      <td>2510</td>\n",
       "      <td>2510</td>\n",
       "      <td>2510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>missing_trapped_or_found_people</th>\n",
       "      <td>402</td>\n",
       "      <td>402</td>\n",
       "      <td>402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>not_related_or_irrelevant</th>\n",
       "      <td>2598</td>\n",
       "      <td>2598</td>\n",
       "      <td>2598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>other_useful_information</th>\n",
       "      <td>6984</td>\n",
       "      <td>6984</td>\n",
       "      <td>6984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prevention</th>\n",
       "      <td>288</td>\n",
       "      <td>288</td>\n",
       "      <td>288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sympathy_and_emotional_support</th>\n",
       "      <td>1969</td>\n",
       "      <td>1969</td>\n",
       "      <td>1969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>treatment</th>\n",
       "      <td>422</td>\n",
       "      <td>422</td>\n",
       "      <td>422</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   _unit_state  tweet_text  \\\n",
       "choose_one_category                                                          \n",
       "affected_people                                            866         866   \n",
       "caution_and_advice                                        1063        1063   \n",
       "deaths_reports                                              93          93   \n",
       "disease_signs_or_symptoms                                  431         431   \n",
       "disease_transmission                                       425         425   \n",
       "displaced_people_and_evacuations                           633         633   \n",
       "donation_needs_or_offers_or_volunteering_services         2610        2610   \n",
       "infrastructure_and_utilities_damage                       1851        1851   \n",
       "injured_or_dead_people                                    2510        2510   \n",
       "missing_trapped_or_found_people                            402         402   \n",
       "not_related_or_irrelevant                                 2598        2598   \n",
       "other_useful_information                                  6984        6984   \n",
       "prevention                                                 288         288   \n",
       "sympathy_and_emotional_support                            1969        1969   \n",
       "treatment                                                  422         422   \n",
       "\n",
       "                                                    cat  \n",
       "choose_one_category                                      \n",
       "affected_people                                     866  \n",
       "caution_and_advice                                 1063  \n",
       "deaths_reports                                       93  \n",
       "disease_signs_or_symptoms                           431  \n",
       "disease_transmission                                425  \n",
       "displaced_people_and_evacuations                    633  \n",
       "donation_needs_or_offers_or_volunteering_services  2610  \n",
       "infrastructure_and_utilities_damage                1851  \n",
       "injured_or_dead_people                             2510  \n",
       "missing_trapped_or_found_people                     402  \n",
       "not_related_or_irrelevant                          2598  \n",
       "other_useful_information                           6984  \n",
       "prevention                                          288  \n",
       "sympathy_and_emotional_support                     1969  \n",
       "treatment                                           422  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file.groupby(['choose_one_category']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#reduced = file[file.choose_one_category == 'injured_or_dead_people']\n",
    "\n",
    "#try with all data points\n",
    "reduced = file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cleaning steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Aid agencies: Vanuatu conditions more challenging than Philippines typhoon http://t.co/G9WIl4qrHx'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduced.iloc[4].tweet_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove twitter specific \n",
    "#file.tweet_text = re.sub(r'http\\S+', '', file.tweet_text)\n",
    "#remove hyperlinks\n",
    "reduced = reduced.replace(to_replace =r'&amp;', value = '', regex = True)\n",
    "reduced = reduced.replace(to_replace =r'&gt;', value = '', regex = True)\n",
    "reduced = reduced.replace(to_replace =r'http\\S+', value = '', regex = True)\n",
    "#remove usernames\n",
    "reduced = reduced.replace(to_replace =r'@\\S+', value = '', regex = True) \n",
    "#remove hashtags\n",
    "#reduced = reduced.replace(to_replace =r'#[A-Za-z0-9]+', value = '', regex = True) \n",
    "# or just remove the hashtag, but leave the actual word\n",
    "reduced = reduced.replace(to_replace ='#', value = '', regex = True) \n",
    "#remove retweet\n",
    "reduced = reduced.replace(to_replace ='RT :', value = '', regex = True) \n",
    "reduced = reduced.replace(to_replace ='RT ', value = '', regex = True) \n",
    "#remove punctation\n",
    "reduced = reduced.replace(to_replace ='[\",:!?\\\\-]', value = ' ', regex = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reduced = reduced.replace(to_replace ='^[[-+]?[0-9]*\\.?[0-9]*]', value = ' ', regex = True)\n",
    "#'^[\\d+\\.$]'\n",
    "#'^[[-+]?[0-9]*\\.?[0-9]*]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Aid agencies  Vanuatu conditions more challenging than Philippines typhoon '"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduced.iloc[4].tweet_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. Tokenize into words (all lower case)\n",
    "reduced.tweet_text = reduced.tweet_text.str.lower()\n",
    "reduced.tweet_text = reduced.tweet_text.str.split() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_stopwords = set(stopwords.words(\"english\"))\n",
    "reduced['tweet_text'] = reduced['tweet_text'].apply(lambda x: [item for item in x if item not in eng_stopwords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#join the list items back to one string\n",
    "reduced['tweet_text'] = reduced['tweet_text'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['_unit_state', 'choose_one_category', 'tweet_text', 'cat'], dtype='object')"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import metrics # for confusion matrix, accuracy score etc\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\\\n",
    "    reduced['tweet_text'], reduced['choose_one_category'], random_state=0, test_size=.2)\n",
    "\n",
    "\n",
    "# CountVectorizer can actucally handle a lot of the preprocessing for us\n",
    "vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                             tokenizer = None,    \\\n",
    "                             preprocessor = None, \\\n",
    "                             stop_words = None,   \\\n",
    "                             max_features = 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 781 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=5000, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Transform the text data to feature\n",
    "# Only fit training data (to mimic real world)\n",
    "\n",
    "vectorizer.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00', '000', '00pm', '01', '011', '02', '03', '04', '05', '06', '07', '08', '09', '10', '100', '1000', '1000s', '1003', '100s', '101', '102', '105', '109', '10km', '10pm', '11', '110', '11am', '11pm', '12', '120', '122', '125', '125mph', '13', '130', '132', '135', '13th', '14', '140', '142', '1490', '15', '150', '1500', '155', '15km', '16', '160', '1670', '17', '170', '1715gmt', '172', '175', '18', '180', '1800', '182', '18km', '19', '1900', '1988', '1989', '1b', '1km', '1m', '1st', '1ã', '20', '200', '2000', '20000', '2005', '2008', '2010', '2012', '2013', '2014', '2015', '20km', '20m', '21', '210', '22', '23', '230', '24', '240', '24km', '25', '250', '26', '260', '27', '28', '285', '29', '294', '2m', '2nd', '2â', '2ã', '30', '300', '31', '32', '3200', '324ff166', '327', '328', '33', '333', '33km', '34', '348', '35', '350', '355', '36', '37', '370', '38', '39', '3km', '3m', '3novices', '3rd', '3ã', '40', '400', '4000', '407', '40km', '41', '42', '43', '44', '440', '449', '45', '450', '46', '460', '47', '4747', '47km', '48', '48km', '49', '4b', '4km', '4m', '4pm', '4th', '4â', '4ã', '50', '500', '5000', '51', '515', '52', '53', '54', '55', '55km', '56', '57', '58', '59', '59mi', '5am', '5km', '5m', '5pm', '5ã', '60', '600', '61', '62', '624', '65', '67', '68', '6am', '6km', '6pm', '70', '700', '70123', '73mi', '74', '74km', '75', '7e', '7km', '7m', '7pm', '7th', '80', '800', '82', '8230', '84', '85', '858', '87', '88', '888', '8km', '8pm', '90', '900', '94', '96km', '977', '99', '9_11', '9news', '9pm', '9ã', '__', '__c', '__f', '__fâ', '__â', '__ã', '__ë', '_a', '_â', '_ã', '_å', 'aand', 'aap', 'ab', 'abandon', 'abandoned', 'abbott', 'abc', 'abc7', 'abcnews', 'abdullah', 'able', 'abp', 'abroad', 'absolute', 'absolutely', 'abt', 'abu', 'abuja', 'ac', 'ac360', 'aca', 'accept', 'accepted', 'accepting', 'access', 'according', 'account', 'accounts', 'accurate', 'ache', 'aches', 'acknowledge', 'acres', 'across', 'act', 'acting', 'action', 'actions', 'active', 'activists', 'activities', 'activity', 'actual', 'actually', 'add', 'added', 'additional', 'address', 'addresses', 'addressing', 'adds', 'adeboye', 'adidas', 'admin', 'administered', 'admits', 'admitted', 'adopt', 'adra', 'advance', 'advice', 'advised', 'advises', 'advisories', 'advisory', 'aerial', 'af', 'afd', 'affairs', 'affect', 'affected', 'affectees', 'affecting', 'affects', 'afghan', 'afghanistan', 'afp', 'afraid', 'africa', 'african', 'africans', 'aft', 'after', 'aftermath', 'afternoon', 'aftershock', 'aftershocks', 'again', 'age', 'agencies', 'agency', 'agenda', 'ago', 'agree', 'ahead', 'ahmadiyya', 'aid', 'aided', 'aids', 'aidã', 'ain', 'air', 'airborne', 'aircraft', 'airforce', 'airlift', 'airlifted', 'airline', 'airlines', 'airport', 'airports', 'ajk', 'akansha_gautam', 'akf', 'aksyonsahagupit', 'al', 'alabama', 'alarm', 'alaska', 'albay', 'album', 'alec', 'alert', 'alerted', 'alerting', 'alerts', 'alertã', 'alex', 'ali', 'alien', 'alive', 'aljazeera', 'all', 'allah', 'allocation', 'allow', 'allowed', 'allowing', 'almighty', 'almost', 'alone', 'along', 'alongside', 'already', 'alright', 'also', 'altaf', 'altafhussain', 'always', 'am', 'amalie', 'aman', 'amazing', 'ambassador', 'amber', 'ambervinson', 'ambulances', 'ameen', 'amen', 'america', 'american', 'americans', 'americas', 'amid', 'amitabh', 'among', 'amount', 'amp', 'an', 'analysis', 'anc', 'ancient', 'and', 'anderson', 'andhra', 'andrew', 'android', 'andã', 'anew', 'ang', 'angeles', 'angels', 'anger', 'angry', 'animal', 'animals', 'animation', 'ann', 'announce', 'announced', 'announcement', 'announces', 'annual', 'another', 'anp', 'answer', 'answered', 'answers', 'anthony', 'anti', 'antibodies', 'anticipate', 'antofagasta', 'anybody', 'anymore', 'anyone', 'anything', 'ap', 'apart', 'apartment', 'app', 'apparently', 'appeal', 'appeals', 'appear', 'appeared', 'appears', 'appetite', 'applaud', 'apple', 'apply', 'appreciate', 'appreciated', 'approach', 'approaches', 'approaching', 'appropriate', 'approval', 'approves', 'approx', 'apr', 'april', 'aquino', 'ar', 'arabia', 'arabian', 'archipelago', 'are', 'area', 'areas', 'argentina', 'arica', 'arizona', 'arm', 'armed', 'army', 'armyrescue', 'armys', 'around', 'arrest', 'arrested', 'arrival', 'arrive', 'arrived', 'arrives', 'arriving', 'art', 'article', 'as', 'asap', 'asha', 'asho', 'ashore', 'asia', 'asian', 'aside', 'ask', 'asked', 'asking', 'asks', 'ass', 'assam', 'assess', 'assessed', 'assesses', 'assessing', 'assessment', 'assist', 'assistance', 'assisting', 'associated', 'assure', 'assures', 'astaghfirullah', 'at', 'ate', 'atlanta', 'atlantic', 'atleast', 'attack', 'attacked', 'attacks', 'attempt', 'attention', 'attitude', 'auckland', 'aug', 'august', 'aur', 'aus', 'auspol', 'australia', 'australian', 'australians', 'authorities', 'authority', 'auto', 'autorickshaw', 'available', 'avalanche', 'averting', 'aviation', 'avoid', 'avoids', 'await', 'awaiting', 'awake', 'awaraan', 'awaran', 'award', 'aware', 'awareness', 'away', 'awesome', 'awful', 'az', 'azad', 'azadi', 'aâ', 'aã', 'b4', 'babies', 'baby', 'bachchan', 'bachelet', 'back', 'bad', 'badly', 'bagh', 'bahrain', 'baja', 'bajacalifornia', 'baldwin', 'ball', 'baloch', 'balochistan', 'balochistanearthquake', 'baltimore', 'baluchistan', 'ban', 'band', 'bangladesh', 'bank', 'bar', 'bare', 'barred', 'barrels', 'base', 'based', 'bash', 'basic', 'basis', 'basketball', 'bat', 'bataan', 'batangas', 'bath', 'bats', 'battered', 'batters', 'battle', 'battling', 'bavi', 'bay', 'bayarea', 'baylor', 'bazar', 'bbc', 'bc', 'bcs', 'be', 'beach', 'bearing', 'bears', 'beat', 'beautiful', 'beauty', 'become', 'becomes', 'becoming', 'bed', 'beef', 'been', 'began', 'begin', 'beginning', 'begins', 'begun', 'behalf', 'behind', 'beings', 'bela', 'believe', 'believed', 'believes', 'ben', 'benefit', 'bepreparedph', 'berlin', 'best', 'bet', 'better', 'beware', 'beyond', 'beã', 'bhaktapur', 'bharat', 'bhutan', 'bhutto', 'bibles', 'bicol', 'bid', 'big', 'bigdata', 'bigger', 'biggest', 'bihar', 'bill', 'billion', 'billions', 'bio', 'biofire', 'biotech', 'birthday', 'bisbee', 'biscuits', 'bishop', 'bit', 'bitch', 'bitter', 'bjp', 'bla', 'black', 'blame', 'blamed', 'blames', 'blaming', 'blankets', 'blast', 'blasts', 'blazed', 'bleeding', 'bless', 'blessed', 'blessings', 'blew', 'blocked', 'blocking', 'blocks', 'blog', 'blogging', 'blood', 'bloody', 'bloomberg', 'blow', 'blown', 'blue', 'bo', 'board', 'boarding', 'boat', 'boats', 'bodies', 'bodily', 'body', 'bolivia', 'bollywood', 'bomb', 'bombing', 'books', 'border', 'borders', 'born', 'borne', 'borongan', 'boston', 'bottles', 'bout', 'box', 'boy', 'boyfriend', 'boys', 'brace', 'braces', 'brampton', 'brave', 'braving', 'bravo', 'brazil', 'breaches', 'break', 'breakfast', 'breaking', 'breakingnews', 'breakingã', 'breaks', 'breakup', 'breath', 'breeding', 'brewing', 'brick', 'bricks', 'bridge', 'bridges', 'brief', 'briefing', 'bring', 'bringbackourmarine', 'bringing', 'brings', 'britain', 'british', 'briton', 'britons', 'brits', 'bro', 'broken', 'brother', 'brothers', 'brought', 'brown', 'bruce', 'bryan_starz', 'bsnl', 'bt', 'btswelcometothephilippines', 'bubblews', 'bucket', 'buckled', 'budget', 'build', 'building', 'buildings', 'built', 'bulacan', 'bullet', 'bulletin', 'bureau', 'buried', 'burma', 'burning', 'bus', 'buses', 'bushes', 'busi', 'business', 'businesses', 'businessweek', 'busy', 'but', 'button', 'buy', 'buying', 'by', 'byã', 'bâ', 'bã', 'ca', 'cabo', 'cabos', 'cabosanlucas', 'cal', 'calabarzon', 'calamities', 'calamity', 'calbayog', 'caledonia', 'cali', 'calif', 'california', 'californiaearthquake', 'californians', 'californiaã', 'call', 'called', 'calling', 'calls', 'calm', 'caloocan', 'caltech', 'came', 'camel', 'camels', 'camera', 'camp', 'campaign', 'camps', 'can', 'canada', 'canadian', 'canadians', 'canberra', 'cancel', 'canceled', 'cancelled', 'cancels', 'cancer', 'cannot', 'cant', 'canyon', 'capacity', 'cape', 'capital', 'capitaltv', 'capsizes', 'captured', 'car', 'care', 'career', 'careful', 'cares', 'caring', 'carnarvon', 'carried', 'carrier', 'carroll', 'carry', 'carrying', 'carson', 'case', 'cases', 'cash', 'caste', 'casualties', 'casualty', 'cat', 'catarman', 'catastrophe', 'catastrophic', 'catbalogan', 'catch', 'category', 'catholic', 'caught', 'cause', 'caused', 'causes', 'causing', 'cautious', 'cavite', 'cbc', 'cbs', 'cc', 'cd', 'cdc', 'cdcchat', 'cebu', 'celebrities', 'celebs', 'cell', 'center', 'centers', 'central', 'centre', 'centres', 'century', 'ceo', 'ch', 'chain', 'chairman', 'challenge', 'challenges', 'chance', 'chances', 'change', 'changed', 'changepenang', 'changes', 'changing', 'channel', 'channels', 'chants', 'chaos', 'charge', 'charged', 'charities', 'charity', 'charlotte', 'chat', 'check', 'checked', 'checkout', 'chenab', 'cheque', 'chest', 'chicago', 'chief', 'child', 'children', 'chile', 'chilean', 'chileearthquake', 'chileearthquakeandtsunami', 'chilequake', 'chileã', 'chili', 'chimney', 'china', 'chinese', 'chiniot', 'choice', 'chpsre', 'christ', 'christian', 'christians', 'church', 'cidrap', 'cities', 'citizen', 'citizens', 'city', 'civic', 'civil', 'claim', 'claims', 'clarify', 'class', 'classes', 'clean', 'cleaning', 'cleanup', 'clear', 'cleared', 'clearing', 'clearly', 'clears', 'cleveland', 'click', 'climate', 'climatechange', 'climb', 'climbed', 'climber', 'climbers', 'climbs', 'clinic', 'clinical', 'clinicians', 'clock', 'close', 'closed', 'closer', 'closure', 'clothes', 'clouds', 'club', 'cm', 'cnn', 'cnnã', 'co', 'coas', 'coast', 'coastal', 'coastline', 'cobb', 'cod']\n"
     ]
    }
   ],
   "source": [
    "# Check that it worked, \n",
    "# now we have fitted a model that can transform features\n",
    "# to sparse matrix representation\n",
    "\n",
    "print(vectorizer.get_feature_names()[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bag = vectorizer.transform(X_train) #transform to a feature matrix\n",
    "test_bag = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18516, 5000)\n",
      "(4629, 5000)\n"
     ]
    }
   ],
   "source": [
    "print(train_bag.toarray().shape) # 20,000 reviews, 2,000 feartures. just as expected\n",
    "print(test_bag.toarray().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 813)\t1\n",
      "  (0, 2131)\t1\n",
      "  (0, 2507)\t1\n",
      "  (0, 3028)\t1\n",
      "  (0, 3062)\t1\n",
      "  (0, 3137)\t1\n",
      "  (0, 3536)\t1\n",
      "  (0, 4670)\t1\n",
      "  (0, 4799)\t1\n",
      "  (1, 1296)\t1\n",
      "  (1, 1367)\t1\n",
      "  (1, 2458)\t1\n",
      "  (1, 3131)\t1\n",
      "  (1, 3247)\t1\n",
      "  (1, 4165)\t1\n",
      "  (2, 13)\t1\n",
      "  (2, 1218)\t2\n",
      "  (2, 1469)\t1\n",
      "  (2, 1470)\t1\n",
      "  (2, 2281)\t1\n",
      "  (2, 2819)\t2\n",
      "  (2, 3546)\t1\n",
      "  (2, 4014)\t1\n",
      "  (2, 4731)\t1\n",
      "  (3, 818)\t1\n",
      "  :\t:\n",
      "  (18513, 4731)\t1\n",
      "  (18514, 1058)\t1\n",
      "  (18514, 1518)\t1\n",
      "  (18514, 2067)\t1\n",
      "  (18514, 2790)\t2\n",
      "  (18514, 3051)\t1\n",
      "  (18514, 3175)\t1\n",
      "  (18514, 3506)\t1\n",
      "  (18514, 4206)\t1\n",
      "  (18514, 4601)\t1\n",
      "  (18514, 4667)\t1\n",
      "  (18514, 4731)\t1\n",
      "  (18515, 92)\t1\n",
      "  (18515, 500)\t1\n",
      "  (18515, 1584)\t1\n",
      "  (18515, 1908)\t1\n",
      "  (18515, 2218)\t1\n",
      "  (18515, 2280)\t1\n",
      "  (18515, 2458)\t1\n",
      "  (18515, 2850)\t1\n",
      "  (18515, 3279)\t1\n",
      "  (18515, 3347)\t1\n",
      "  (18515, 3660)\t1\n",
      "  (18515, 4477)\t1\n",
      "  (18515, 4504)\t1\n"
     ]
    }
   ],
   "source": [
    "type(train_bag) # sparse matrix representation\n",
    "\n",
    "print(train_bag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasify with Random Forest model\n",
    "\n",
    "* Fit a Random Forest model to our bagged data set in order to do the sentiment analysis on `review_clean_original` and print the **validation accuracy** by using `forest.predict(test_bag)` and then comparing the resulting sentiment predictions with the ones stored in `y_test`.\n",
    "\n",
    "*This can take 2-3 mins to run*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['_unit_state', 'choose_one_category', 'tweet_text', 'cat'], dtype='object')"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduced.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list(reduced.choose_one_category.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Florian\\Anaconda3\\envs\\data-x\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "## Initialize a Random Forest classifier with 50 trees\n",
    "# hyperparameter n_estimators always set in instantiation\n",
    "\n",
    "forest = RandomForestClassifier(n_estimators = 50) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 57 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Fit the forest to the training set, using the bag of words as \n",
    "# features and the sentiment labels as the target variable\n",
    "\n",
    "forest = forest.fit(train_bag, y_train) # can take 60 seconds to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "\n",
    "train_predictions = forest.predict(train_bag)\n",
    "valid_predictions = forest.predict(test_bag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9740224670555195"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.accuracy_score(y_train,train_predictions) # 100% training accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.700151220565997"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.accuracy_score(y_test,valid_predictions) # 83% test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 126,    0,    0,   11,    3,    0,    0,    0,    0,    0,    1,\n",
       "          13,    1,    1,   10],\n",
       "       [   0,   64,    0,    0,    0,    0,    4,    4,    1,    0,   12,\n",
       "         116,    0,   13,    0],\n",
       "       [   2,    0,    5,    2,    0,    0,    0,    0,    1,    0,    0,\n",
       "           1,    0,    0,    1],\n",
       "       [  12,    0,    0,   61,    2,    0,    0,    0,    0,    0,    0,\n",
       "           9,    0,    0,    2],\n",
       "       [   3,    0,    0,    1,   65,    0,    0,    0,    0,    0,    0,\n",
       "          13,    2,    0,    1],\n",
       "       [   0,    4,    0,    0,    0,   56,   11,    9,    4,    2,    4,\n",
       "          31,    0,    0,    0],\n",
       "       [   0,    3,    0,    0,    0,    3,  408,    8,    0,    1,   19,\n",
       "          84,    0,    6,    0],\n",
       "       [   0,    3,    0,    0,    0,    2,   10,  245,    8,    0,   11,\n",
       "          97,    0,    0,    0],\n",
       "       [   0,    1,    0,    0,    0,    0,    7,    3,  423,    2,    8,\n",
       "          35,    0,    3,    0],\n",
       "       [   0,    0,    0,    0,    0,    9,   14,    3,    3,   25,    7,\n",
       "          25,    0,    4,    0],\n",
       "       [   0,    2,    0,    0,    1,    0,   20,   11,   17,    1,  331,\n",
       "         102,    3,   12,    0],\n",
       "       [   6,   23,    3,    3,   10,    9,   81,   69,   14,    4,   91,\n",
       "        1054,    7,   24,    7],\n",
       "       [   2,    0,    0,    1,    1,    0,    0,    0,    0,    0,    1,\n",
       "          19,   33,    0,    3],\n",
       "       [   0,    4,    0,    0,    0,    1,   39,    5,    4,    0,   33,\n",
       "          51,    0,  280,    0],\n",
       "       [   2,    0,    0,    4,    1,    0,    1,    0,    0,    0,    2,\n",
       "           6,    2,    0,   65]], dtype=int64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Confusion matrix\n",
    "# Is the number of False Positives and True negatives approx 50/50?\n",
    "display(metrics.confusion_matrix(y_test,valid_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['affected_people', 'caution_and_advice', 'deaths_reports',\n",
       "       'disease_signs_or_symptoms', 'disease_transmission',\n",
       "       'displaced_people_and_evacuations',\n",
       "       'donation_needs_or_offers_or_volunteering_services',\n",
       "       'infrastructure_and_utilities_damage', 'injured_or_dead_people',\n",
       "       'missing_trapped_or_found_people', 'not_related_or_irrelevant',\n",
       "       'other_useful_information', 'prevention',\n",
       "       'sympathy_and_emotional_support', 'treatment'], dtype=object)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1669</th>\n",
       "      <td>path hurricane odile's destruction seen instag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1991</th>\n",
       "      <td>(losdelsonido) hurricane odile slams mexico's ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>532</th>\n",
       "      <td>news california even buildings seismic retrofi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1500</th>\n",
       "      <td>even buildings seismic retrofits damaged napa ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1748</th>\n",
       "      <td>100 000+ people likely homeless 90% houses dam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1743</th>\n",
       "      <td>awful damage hurricane odile baja california</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>974</th>\n",
       "      <td>tragic sight los cabos international airport h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>845</th>\n",
       "      <td>napaquake damage. broken chimneys problems ktv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>653</th>\n",
       "      <td>us quake damage ã¢â¬å rock on. california th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>508</th>\n",
       "      <td>cabosanlucas damage pics dallas residents jean...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1291</th>\n",
       "      <td>today's news photo hurricane odile damage cabo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>691</th>\n",
       "      <td>hurricane odile slams baja california; extensi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>535</th>\n",
       "      <td>. oops photo airport los cabos severely damage...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1184</th>\n",
       "      <td>believe 6.0 earthquake california damage winer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1528</th>\n",
       "      <td>path hurricane odile's destruction seen instag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1445</th>\n",
       "      <td>path hurricane odile's destruction seen instag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>528</th>\n",
       "      <td>real damage cabo hurricaneodile. baja stormtra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>floods ukfloods monsoon floods hit uttar prade...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>fortunately gallery 1870 yountville ca incurre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>633</th>\n",
       "      <td>hurricaneodile damage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1174</th>\n",
       "      <td>jk ground report floods leave behind damage de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1433</th>\n",
       "      <td>hurricane odile slams baja california; extensi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1816</th>\n",
       "      <td>flakoboow hurricane odile slams mexico's baja ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>video hurricane odile damages mexico's baja ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1828</th>\n",
       "      <td>even buildings seismic retrofits damaged napa ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1704</th>\n",
       "      <td>anyone staying near hotelelganzo sanjosedelcab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>728</th>\n",
       "      <td>cabo news | hotels damaged odile |</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1355</th>\n",
       "      <td>path hurricane odile's destruction seen instagram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1414</th>\n",
       "      <td>hurricane odile blazed trail destruction mexic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>623</th>\n",
       "      <td>napa wineries sustain damage 6.1 magnitude ear...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>631</th>\n",
       "      <td>storm caused severe damage airport los cabos p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>550</th>\n",
       "      <td>wine curse seismic activity via california qua...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1717</th>\n",
       "      <td>path hurricane odile's destruction seen instag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>hurricane odile damaged parts hotels structura...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1484</th>\n",
       "      <td>hurricane odile causes extensive property dama...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1273</th>\n",
       "      <td>jammu kashmir highway got ravaged due floods r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>ã¢â¬å photos damage caused hurricane odile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514</th>\n",
       "      <td>hurricane odile damaged mexico's fishing farmi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>872</th>\n",
       "      <td>hurricane damage severe heading cabo soon airp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>643</th>\n",
       "      <td>children living shelter freshwota school vanua...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>napaquake ruptures water mains gas lines 3 cri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1382</th>\n",
       "      <td>01molleto path hurricane odile's destruction s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1944</th>\n",
       "      <td>mashable path hurricane odile's destruction se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>985</th>\n",
       "      <td>mx westin los cabos sheraton hacienda suffered...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367</th>\n",
       "      <td>srinagar leh national highway connecting flood...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>path hurricane odile's destruction seen instag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1175</th>\n",
       "      <td>path hurricane odile's destruction seen instagram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>892</th>\n",
       "      <td>hurricane odile slams mexico's baja california...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699</th>\n",
       "      <td>napa valley vintners still assessing damage ea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1506</th>\n",
       "      <td>path hurricane odile's destruction seen instagram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1085</th>\n",
       "      <td>sammyhagar cancelled birthday bash shows cabo ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1087</th>\n",
       "      <td>napa authorities use rc copter inspect damaged...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>significant hurricane damage... | ã¢â¬å hote...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1945</th>\n",
       "      <td>las cabos international airport cabo san lucas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1267</th>\n",
       "      <td>local officials estimated monday napa valley s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1734</th>\n",
       "      <td>[bw] small winemakers assess napa quake damage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>seeing first signs damage bad. hotel next hurr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>path hurricane odile's destruction baja penins...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1749</th>\n",
       "      <td>bbc news hurricane odile damages mexico's baja...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>682</th>\n",
       "      <td>raw hurricane odile damages landfall hits baja...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>245 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             tweet_text\n",
       "1669  path hurricane odile's destruction seen instag...\n",
       "1991  (losdelsonido) hurricane odile slams mexico's ...\n",
       "532   news california even buildings seismic retrofi...\n",
       "1500  even buildings seismic retrofits damaged napa ...\n",
       "1748  100 000+ people likely homeless 90% houses dam...\n",
       "1743       awful damage hurricane odile baja california\n",
       "974   tragic sight los cabos international airport h...\n",
       "845   napaquake damage. broken chimneys problems ktv...\n",
       "653   us quake damage ã¢â¬å rock on. california th...\n",
       "508   cabosanlucas damage pics dallas residents jean...\n",
       "1291  today's news photo hurricane odile damage cabo...\n",
       "691   hurricane odile slams baja california; extensi...\n",
       "535   . oops photo airport los cabos severely damage...\n",
       "1184  believe 6.0 earthquake california damage winer...\n",
       "1528  path hurricane odile's destruction seen instag...\n",
       "1445  path hurricane odile's destruction seen instag...\n",
       "528   real damage cabo hurricaneodile. baja stormtra...\n",
       "109   floods ukfloods monsoon floods hit uttar prade...\n",
       "67    fortunately gallery 1870 yountville ca incurre...\n",
       "633                               hurricaneodile damage\n",
       "1174  jk ground report floods leave behind damage de...\n",
       "1433  hurricane odile slams baja california; extensi...\n",
       "1816  flakoboow hurricane odile slams mexico's baja ...\n",
       "56    video hurricane odile damages mexico's baja ca...\n",
       "1828  even buildings seismic retrofits damaged napa ...\n",
       "1704  anyone staying near hotelelganzo sanjosedelcab...\n",
       "728                  cabo news | hotels damaged odile |\n",
       "1355  path hurricane odile's destruction seen instagram\n",
       "1414  hurricane odile blazed trail destruction mexic...\n",
       "623   napa wineries sustain damage 6.1 magnitude ear...\n",
       "...                                                 ...\n",
       "631   storm caused severe damage airport los cabos p...\n",
       "550   wine curse seismic activity via california qua...\n",
       "1717  path hurricane odile's destruction seen instag...\n",
       "225   hurricane odile damaged parts hotels structura...\n",
       "1484  hurricane odile causes extensive property dama...\n",
       "1273  jammu kashmir highway got ravaged due floods r...\n",
       "143        ã¢â¬å photos damage caused hurricane odile\n",
       "514   hurricane odile damaged mexico's fishing farmi...\n",
       "872   hurricane damage severe heading cabo soon airp...\n",
       "643   children living shelter freshwota school vanua...\n",
       "321   napaquake ruptures water mains gas lines 3 cri...\n",
       "1382  01molleto path hurricane odile's destruction s...\n",
       "1944  mashable path hurricane odile's destruction se...\n",
       "985   mx westin los cabos sheraton hacienda suffered...\n",
       "367   srinagar leh national highway connecting flood...\n",
       "260   path hurricane odile's destruction seen instag...\n",
       "1175  path hurricane odile's destruction seen instagram\n",
       "892   hurricane odile slams mexico's baja california...\n",
       "699   napa valley vintners still assessing damage ea...\n",
       "1506  path hurricane odile's destruction seen instagram\n",
       "1085  sammyhagar cancelled birthday bash shows cabo ...\n",
       "1087  napa authorities use rc copter inspect damaged...\n",
       "159   significant hurricane damage... | ã¢â¬å hote...\n",
       "1945  las cabos international airport cabo san lucas...\n",
       "1267  local officials estimated monday napa valley s...\n",
       "1734     [bw] small winemakers assess napa quake damage\n",
       "295   seeing first signs damage bad. hotel next hurr...\n",
       "39    path hurricane odile's destruction baja penins...\n",
       "1749  bbc news hurricane odile damages mexico's baja...\n",
       "682   raw hurricane odile damages landfall hits baja...\n",
       "\n",
       "[245 rows x 1 columns]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What are the characteristics of right / wrong classifications\n",
    "# Good practice when doing analysis\n",
    "\n",
    "df_test = pd.DataFrame(X_test)\n",
    "df_test[(y_test.values=='infrastructure_and_utilities_damage') & (valid_predictions=='infrastructure_and_utilities_damage')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.23302134e-06 3.80829646e-04 3.74849619e-05 ... 1.34828742e-04\n",
      " 2.61403472e-05 4.77084384e-05]\n"
     ]
    }
   ],
   "source": [
    "importances = forest.feature_importances_\n",
    "# returns relative importance of all features.\n",
    "# they are in the order of the columns\n",
    "print(importances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature ranking:\n",
      "['damage', 'injured', 'dozens', 'photo', 'earthquake', 'help', 'california', 'napa', 'injuries', 'hurt']\n"
     ]
    }
   ],
   "source": [
    "# sort importance scores\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "top_10 = indices[:10]\n",
    "\n",
    "# Get top ten features\n",
    "print([vectorizer.get_feature_names()[ind] for ind in top_10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WIP: Method - to be adjusted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# put everything together in a function\n",
    "\n",
    "def predict_sentiment(cleaned_reviews, y=train[\"sentiment\"]):\n",
    "\n",
    "    print(\"Creating the bag of words model!\\n\")\n",
    "    # CountVectorizer\" is scikit-learn's bag of words tool, here we show more keywords \n",
    "    vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                                 tokenizer = None,    \\\n",
    "                                 preprocessor = None, \\\n",
    "                                 stop_words = None,   \\\n",
    "                                 max_features = 2000) \n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\\\n",
    "    cleaned_reviews, y, random_state=0, test_size=.2)\n",
    "\n",
    "    # Then we use fit_transform() to fit the model / learn the vocabulary,\n",
    "    # then transform the data into feature vectors.\n",
    "    # The input should be a list of strings. .toarraty() converts to a numpy array\n",
    "    \n",
    "    train_bag = vectorizer.fit_transform(X_train).toarray()\n",
    "    test_bag = vectorizer.transform(X_test).toarray()\n",
    "\n",
    "    # You can extract the vocabulary created by CountVectorizer\n",
    "    # by running print(vectorizer.get_feature_names())\n",
    "\n",
    "\n",
    "    print(\"Training the random forest classifier!\\n\")\n",
    "    # Initialize a Random Forest classifier with 75 trees\n",
    "    forest = RandomForestClassifier(n_estimators = 50) \n",
    "\n",
    "    # Fit the forest to the training set, using the bag of words as \n",
    "    # features and the sentiment labels as the target variable\n",
    "    forest = forest.fit(train_bag, y_train)\n",
    "\n",
    "\n",
    "    train_predictions = forest.predict(train_bag)\n",
    "    test_predictions = forest.predict(test_bag)\n",
    "    \n",
    "    train_acc = metrics.accuracy_score(y_train, train_predictions)\n",
    "    valid_acc = metrics.accuracy_score(y_test, test_predictions)\n",
    "    print(\"The training accuracy is: \", train_acc, \"\\n\", \"The validation accuracy is: \", valid_acc)\n",
    "    \n",
    "    return(forest,vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ideabox below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Original cleaned to lemmatized and stemmed data set\n",
    "\n",
    "Now carry out the same analysis as above but on the `review_clean_ps` and `review_clean_wnl`. \n",
    "\n",
    "What data preprocessing strategy worked the best? Why do you think that is? (Feel free to change the number of features extracted in the bag of words model and the number of trees in the random forest model (i.e. the hyperparameters in our model), to see how it effects your accuracy. Is the accuracy better or worse?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "print('Original Reviews')\n",
    "forest1,vec1 = predict_sentiment(review_clean_original)\n",
    "print('Porter Stemmer')\n",
    "forest2,vec2 = predict_sentiment(review_clean_ps)\n",
    "print('Lemmatizing')\n",
    "forest3,vec3 = predict_sentiment(review_clean_wnl)\n",
    "\n",
    "\n",
    "# It  seems like Porter Stemmer and Lemmatizing does not effect the results as much as we thought\n",
    "# This is just what Sebastian Raschka points out in his book Python Machine Learning:\n",
    "\n",
    "'''\n",
    "The Porter stemming algorithm is probably the oldest and simplest\n",
    "stemming algorithm. Other popular stemming algorithms include the\n",
    "newer Snowball stemmer (Porter2 or \"English\" stemmer) or the Lancaster\n",
    "stemmer (Paice-Husk stemmer), which is faster but also more aggressive\n",
    "than the Porter stemmer. Those alternative stemming algorithms are also\n",
    "available through the NLTK package (http://www.nltk.org/api/\n",
    "nltk.stem.html).\n",
    "\n",
    "While stemming can create non-real words, such as thu, (from thus) as\n",
    "shown in the previous example, a technique called lemmatization aims to\n",
    "obtain the canonical (grammatically correct) forms of individual words—\n",
    "the so-called lemmas. However, lemmatization is computationally more\n",
    "diffcult and expensive compared to stemming and, in practice, it has\n",
    "been observed that stemming and lemmatization have little impact on the\n",
    "performance of text classifcation (Michal Toman, Roman Tesar, and Karel\n",
    "Jezek. Infuence of word normalization on text classifcation. Proceedings of\n",
    "InSciT, pages 354–358, 2006).\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec1.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for vectorizer in [vec1, vec2, vec3]:\n",
    "    print('TOP TEN IMPORTANT FEATURES:')\n",
    "    importances = forest.feature_importances_\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    top_10 = indices[:10]\n",
    "    print([vectorizer.get_feature_names()[ind] for ind in top_10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named entity recognition/ disambiguiation\n",
    "- find out name of school, city, street etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis\n",
    "- - sentiment analysis \n",
    "    - check paper at https://www.analyticsvidhya.com/blog/2017/01/sentiment-analysis-of-twitter-posts-on-chennai-floods-using-python/, where sentiment analysis was performed on Chennai flood dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output: counting expressions (like Sandy Hook School or shooting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return 'n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi sandy hook school i think there be somebody shoot in here in sandy hook school because somebody get a gun i catch a glimpse of someone theyre run down the hallway they be still run theyre still shoot sandy hook school please\n"
     ]
    }
   ],
   "source": [
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "wnl_stems = []\n",
    "for pair in token_tag:\n",
    "    res = wnl.lemmatize(pair[0],pos=get_wordnet_pos(pair[1]))\n",
    "    wnl_stems.append(res)\n",
    "\n",
    "print(' '.join(wnl_stems))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REVIEW WITHOUT STOPWORDS:\n",
      "hi sandy hook school think somebody shooting sandy hook school somebodys got gun caught glimpse someone theyre running hallway still running theyre still shooting sandy hook school please\n",
      "\n",
      "Stop words removed ['i', 'there', 'is', 'in', 'here', 'in', 'because', 'a', 'i', 'a', 'of', 'down', 'the', 'they', 'are']\n",
      "\n",
      "NUMBER OF STOPWORDS REMOVED: 15\n"
     ]
    }
   ],
   "source": [
    "tsc_wo_stopwords = [w for w in tsc_words if not w in stopwords.words(\"english\")]\n",
    "removed_stopwords = [w for w in tsc_words if w in stopwords.words(\"english\")]\n",
    "\n",
    "print('REVIEW WITHOUT STOPWORDS:')\n",
    "print(' '.join(tsc_wo_stopwords))\n",
    "print()\n",
    "print('Stop words removed', removed_stopwords)\n",
    "print()\n",
    "print('NUMBER OF STOPWORDS REMOVED:',len(removed_stopwords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### here follows a summary of what we extracted from the text (summary, keywords etc.) and how this influences the priority"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### processing: spot what is an emergency situation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### recommend steps what and how to do it --> what to employ and where to employ to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### some more links about what we can do\n",
    "- https://blog.paralleldots.com/research/artificial-intelligence-can-make-public-transportation-safer/?source=post_page---------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
