{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import bs4 as bs\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize # tokenizes sentences\n",
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "#Machine Learning\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import metrics # for confusion matrix, accuracy score etc\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start dataset loading: \n",
      "Datasets ready to use!\n",
      "Total Length of DataFrame is 23145\n",
      "\n",
      " Study the choose_one_category\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_unit_state</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>cat</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>choose_one_category</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>affected_people</th>\n",
       "      <td>866</td>\n",
       "      <td>866</td>\n",
       "      <td>866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>caution_and_advice</th>\n",
       "      <td>1063</td>\n",
       "      <td>1063</td>\n",
       "      <td>1063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>deaths_reports</th>\n",
       "      <td>93</td>\n",
       "      <td>93</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disease_signs_or_symptoms</th>\n",
       "      <td>431</td>\n",
       "      <td>431</td>\n",
       "      <td>431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disease_transmission</th>\n",
       "      <td>425</td>\n",
       "      <td>425</td>\n",
       "      <td>425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>displaced_people_and_evacuations</th>\n",
       "      <td>633</td>\n",
       "      <td>633</td>\n",
       "      <td>633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>donation_needs_or_offers_or_volunteering_services</th>\n",
       "      <td>2610</td>\n",
       "      <td>2610</td>\n",
       "      <td>2610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>infrastructure_and_utilities_damage</th>\n",
       "      <td>1851</td>\n",
       "      <td>1851</td>\n",
       "      <td>1851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>injured_or_dead_people</th>\n",
       "      <td>2510</td>\n",
       "      <td>2510</td>\n",
       "      <td>2510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>missing_trapped_or_found_people</th>\n",
       "      <td>402</td>\n",
       "      <td>402</td>\n",
       "      <td>402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>not_related_or_irrelevant</th>\n",
       "      <td>2598</td>\n",
       "      <td>2598</td>\n",
       "      <td>2598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>other_useful_information</th>\n",
       "      <td>6984</td>\n",
       "      <td>6984</td>\n",
       "      <td>6984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prevention</th>\n",
       "      <td>288</td>\n",
       "      <td>288</td>\n",
       "      <td>288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sympathy_and_emotional_support</th>\n",
       "      <td>1969</td>\n",
       "      <td>1969</td>\n",
       "      <td>1969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>treatment</th>\n",
       "      <td>422</td>\n",
       "      <td>422</td>\n",
       "      <td>422</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   _unit_state  tweet_text  \\\n",
       "choose_one_category                                                          \n",
       "affected_people                                            866         866   \n",
       "caution_and_advice                                        1063        1063   \n",
       "deaths_reports                                              93          93   \n",
       "disease_signs_or_symptoms                                  431         431   \n",
       "disease_transmission                                       425         425   \n",
       "displaced_people_and_evacuations                           633         633   \n",
       "donation_needs_or_offers_or_volunteering_services         2610        2610   \n",
       "infrastructure_and_utilities_damage                       1851        1851   \n",
       "injured_or_dead_people                                    2510        2510   \n",
       "missing_trapped_or_found_people                            402         402   \n",
       "not_related_or_irrelevant                                 2598        2598   \n",
       "other_useful_information                                  6984        6984   \n",
       "prevention                                                 288         288   \n",
       "sympathy_and_emotional_support                            1969        1969   \n",
       "treatment                                                  422         422   \n",
       "\n",
       "                                                    cat  \n",
       "choose_one_category                                      \n",
       "affected_people                                     866  \n",
       "caution_and_advice                                 1063  \n",
       "deaths_reports                                       93  \n",
       "disease_signs_or_symptoms                           431  \n",
       "disease_transmission                                425  \n",
       "displaced_people_and_evacuations                    633  \n",
       "donation_needs_or_offers_or_volunteering_services  2610  \n",
       "infrastructure_and_utilities_damage                1851  \n",
       "injured_or_dead_people                             2510  \n",
       "missing_trapped_or_found_people                     402  \n",
       "not_related_or_irrelevant                          2598  \n",
       "other_useful_information                           6984  \n",
       "prevention                                          288  \n",
       "sympathy_and_emotional_support                     1969  \n",
       "treatment                                           422  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Study the cat\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_unit_state</th>\n",
       "      <th>choose_one_category</th>\n",
       "      <th>tweet_text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cat</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>eq</th>\n",
       "      <td>9057</td>\n",
       "      <td>9057</td>\n",
       "      <td>9057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>flood</th>\n",
       "      <td>4016</td>\n",
       "      <td>4016</td>\n",
       "      <td>4016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>storm</th>\n",
       "      <td>6039</td>\n",
       "      <td>6039</td>\n",
       "      <td>6039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>virus</th>\n",
       "      <td>4033</td>\n",
       "      <td>4033</td>\n",
       "      <td>4033</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       _unit_state  choose_one_category  tweet_text\n",
       "cat                                                \n",
       "eq            9057                 9057        9057\n",
       "flood         4016                 4016        4016\n",
       "storm         6039                 6039        6039\n",
       "virus         4033                 4033        4033"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = data_preprocessing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example before cleaning at position 10\n",
      "Thousands await aid in Vanuatu as death toll increases - Fox News http://t.co/RZzbiN1VwK\n",
      "example after cleaning at position 10\n",
      "thousands await aid in vanuatu as death toll increases   fox news \n"
     ]
    }
   ],
   "source": [
    "# df_clean method default settings hashtag_just_sign=True, remove_stopwords=True, test_position=4\n",
    "df_clean = tweet_cleaner(df, remove_stopwords=False, test_position=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train on content: 'other_useful_information', 'infrastructure_and_utilities_damage',\n",
    "       'injured_or_dead_people', 'not_related_or_irrelevant',\n",
    "       'donation_needs_or_offers_or_volunteering_services',\n",
    "       'caution_and_advice', 'sympathy_and_emotional_support',\n",
    "       'missing_trapped_or_found_people',\n",
    "       'displaced_people_and_evacuations', 'affected_people',\n",
    "       'disease_signs_or_symptoms', 'prevention', 'disease_transmission',\n",
    "       'treatment', 'deaths_reports'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing ...\n",
      "                         \n",
      "Train forest for  other_useful_information\n",
      "---------------------------\n",
      "Creating the bag of words model!\n",
      "\n",
      "Training the random forest classifier!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output_content, output_content_importances, output_content_feature_names = training_on_content(df_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train on cat: 'storm', 'eq', 'flood', 'virus'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output_category, output_cat_importances, output_cat_feature_names = training_on_cat(df_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output the lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output_content)\n",
    "print(output_cat_importances)\n",
    "#control output that we put out the identical list in both cases\n",
    "print(output_cat_feature_names[1][100:110])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output)\n",
    "print(output_content_importances)\n",
    "#control output that we put out the identical list in both cases\n",
    "print(output_content_feature_names[1][100:110])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Twitter datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "source data from https://crisisnlp.qcri.org/lrec2016/lrec2016.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data presented by:\n",
    "- Muhammad Imran, Prasenjit Mitra, Carlos Castillo: Twitter as a Lifeline: Human-annotated Twitter Corpora for NLP of Crisis-related Messages. In Proceedings of the 10th Language Resources and Evaluation Conference (LREC), pp. 1638-1643. May 2016, Portorož, Slovenia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing():\n",
    "    # eq - earthquake\n",
    "\n",
    "    print('Start dataset loading: ')\n",
    "    eq_pakistan_2013 = pd.read_csv('data/2013_pakistan_eq.csv', skip_blank_lines=True, encoding = \"ISO-8859-1\")\n",
    "    eq_pakistan_2013['cat'] = 'eq'\n",
    "\n",
    "    eq_california_2014 = pd.read_csv('data/2014_california_eq.csv', skip_blank_lines=True, encoding = \"ISO-8859-1\")\n",
    "    eq_california_2014['cat'] = 'eq'\n",
    "\n",
    "    eq_chile_2014 = pd.read_csv('data/2014_chile_eq_en.csv', skip_blank_lines=True, encoding = \"ISO-8859-1\")\n",
    "    eq_chile_2014['cat'] = 'eq'\n",
    "\n",
    "    ebola_virus_2014 = pd.read_csv('data/2014_ebola_virus.csv', skip_blank_lines=True, encoding = \"ISO-8859-1\")\n",
    "    ebola_virus_2014['cat'] = 'virus'\n",
    "\n",
    "    hurricane_odile_2014 = pd.read_csv('data/2014_hurricane_odile.csv', skip_blank_lines=True, encoding = \"ISO-8859-1\")\n",
    "    hurricane_odile_2014['cat'] = 'storm'\n",
    "\n",
    "    flood_india_2014 = pd.read_csv('data/2014_india_floods.csv', skip_blank_lines=True, encoding = \"ISO-8859-1\")\n",
    "    flood_india_2014['cat'] = 'flood'\n",
    "\n",
    "    middle_east_respiratory_2014 = pd.read_csv('data/2014_mers_cf_labels.csv', skip_blank_lines=True, encoding = \"ISO-8859-1\")\n",
    "    middle_east_respiratory_2014['cat'] = 'virus'\n",
    "\n",
    "    flood_pakistan_2014 = pd.read_csv('data/2014_pakistan_floods_cf_labels.csv', skip_blank_lines=True, encoding = \"ISO-8859-1\")\n",
    "    flood_pakistan_2014['cat'] = 'flood'\n",
    "\n",
    "    typhoon_philippines_2014 = pd.read_csv('data/2014_typhoon_hagupit_cf_labels.csv', skip_blank_lines=True, encoding = \"ISO-8859-1\")\n",
    "    typhoon_philippines_2014['cat']='storm'\n",
    "\n",
    "    cyclone_pam_2015 = pd.read_csv('data/2015_cyclone_pam_cf_labels.csv', skip_blank_lines=True, encoding = \"ISO-8859-1\")\n",
    "    cyclone_pam_2015['cat'] = 'storm'\n",
    "\n",
    "    eq_nepal_2015 = pd.read_csv('data/2015_nepal_eq_cf_labels.csv', skip_blank_lines=True, encoding = \"ISO-8859-1\")\n",
    "    eq_nepal_2015['cat'] = 'eq'\n",
    "    \n",
    "    #make it one dataframe\n",
    "    df = pd.concat([cyclone_pam_2015, eq_nepal_2015, typhoon_philippines_2014, flood_pakistan_2014,\n",
    "                middle_east_respiratory_2014, flood_india_2014, hurricane_odile_2014, \n",
    "                ebola_virus_2014, eq_chile_2014, eq_california_2014, eq_pakistan_2013])\n",
    "    \n",
    "    #drop unneccessary columns\n",
    "    df = df.drop(['_unit_id', '_golden', '_trusted_judgments',\n",
    "       '_last_judgment_at', 'choose_one_category:confidence', 'choose_one_category_gold',\n",
    "       'tweet_id'], axis=1)\n",
    "    \n",
    "    #drop nan values\n",
    "    df = df.dropna()\n",
    "    \n",
    "    print('Datasets ready to use!')\n",
    "    \n",
    "    print('Total Length of DataFrame is', len(df))\n",
    "    \n",
    "    print('\\n Study the choose_one_category')\n",
    "    display(df.groupby(['choose_one_category']).count())\n",
    "    \n",
    "    print('\\n Study the cat')\n",
    "    display(df.groupby(['cat']).count())\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cleaning steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_cleaner (dfc, hashtag_just_sign=True, remove_stopwords=True, test_position=4):  \n",
    "    #check one example before cleaning\n",
    "    print('example before cleaning at position', test_position)\n",
    "    print(dfc.iloc[test_position].tweet_text)\n",
    "\n",
    "    #IN ANY CASE \n",
    "    #non alpabetical/ numerical\n",
    "    dfc = dfc.replace(to_replace =r'&amp;', value = '', regex = True)\n",
    "    dfc = dfc.replace(to_replace =r'&gt;', value = '', regex = True)\n",
    "    #hyperlinks\n",
    "    dfc = dfc.replace(to_replace =r'http\\S+', value = '', regex = True)\n",
    "    #usernames\n",
    "    dfc = dfc.replace(to_replace =r'@\\S+', value = '', regex = True) \n",
    "    #remove retweet\n",
    "    dfc = dfc.replace(to_replace ='RT :', value = '', regex = True) \n",
    "    dfc = dfc.replace(to_replace ='RT ', value = '', regex = True)\n",
    "    \n",
    "    \n",
    "    #HASHTAG OPTIONAL REMOVE\n",
    "    if(hashtag_just_sign == True):\n",
    "        dfc = dfc.replace(to_replace ='#', value = '', regex = True) \n",
    "    else:\n",
    "        dfc = dfc.replace(to_replace =r'#\\S+', value = '', regex = True)\n",
    "        \n",
    "    #IN ANY CASE\n",
    "    #remove punctation\n",
    "    dfc = dfc.replace(to_replace ='[\",:!?\\\\-]', value = ' ', regex = True)\n",
    "    #4. Tokenize into words (all lower case)\n",
    "    dfc.tweet_text = dfc.tweet_text.str.lower()\n",
    "    #make sure no weird letters left\n",
    "    \n",
    "    #u = df.select_dtypes(object)\n",
    "    dfc['tweet_text'] = dfc['tweet_text'].apply(lambda x: x.encode('ascii', 'ignore').decode('ascii'))\n",
    "    \n",
    "    if(remove_stopwords == True):\n",
    "        dfc.tweet_text = dfc.tweet_text.str.split() \n",
    "        eng_stopwords = set(stopwords.words(\"english\"))\n",
    "        dfc['tweet_text'] = dfc['tweet_text'].apply(lambda x: [item for item in x if item not in eng_stopwords])\n",
    "        #join the list items back to one string\n",
    "        dfc['tweet_text'] = dfc['tweet_text'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "    \n",
    "    \n",
    "    print('example after cleaning at position', test_position)\n",
    "    print(dfc.iloc[test_position].tweet_text)\n",
    "    \n",
    "    return dfc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file=df_clean.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_on_content(file):\n",
    "    content = list(file.choose_one_category.unique())\n",
    "    content_importances = list()\n",
    "    content_feature_names = list()\n",
    "\n",
    "    for con in content:\n",
    "        #build the needed dataset\n",
    "        temp_file = file.copy(deep=True)\n",
    "        replacing_list = list()\n",
    "        replacing_list = content.copy()\n",
    "        replacing_list.remove(con)\n",
    "        #print(replacing_list)\n",
    "        string_to_be_added = str('no_'+ con)\n",
    "        #print(string_to_be_added)\n",
    "        print('testing ...')\n",
    "        #print(replacing_list)\n",
    "        temp_file.choose_one_category = temp_file.choose_one_category.replace(to_replace=replacing_list, value=string_to_be_added) \n",
    "        print('                         ')\n",
    "        print('Train forest for ', con)\n",
    "        print('---------------------------')\n",
    "        forest_out, vectorizer_out, feature_names_out, importances_out = train_forest(temp_file['tweet_text'], temp_file['choose_one_category'])\n",
    "        content_feature_names.append(feature_names_out)\n",
    "        content_importances.append(importances_out)\n",
    "\n",
    "    print('DONE with Training on Content!!!')\n",
    "    \n",
    "    return conent, content_importances, content_feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_on_cat(file):    \n",
    "    category = list(file.cat.unique())\n",
    "    cat_importances = list()\n",
    "    cat_feature_names = list()\n",
    "\n",
    "    for cat in category:\n",
    "        #build the needed dataset\n",
    "        temp_file = file.copy(deep=True)\n",
    "        replacing_list = list()\n",
    "        replacing_list = category.copy()\n",
    "        replacing_list.remove(cat)\n",
    "        #print(replacing_list)\n",
    "        string_to_be_added = str('no_'+ cat)\n",
    "        #print(string_to_be_added)\n",
    "        print('testing ...')\n",
    "        #print(replacing_list)\n",
    "        temp_file.cat = temp_file.cat.replace(to_replace=replacing_list, value=string_to_be_added) \n",
    "        print('                         ')\n",
    "        print('Train forest for ', cat)\n",
    "        print('---------------------------')\n",
    "        forest_out, vectorizer_out, feature_names_out, importances_out = train_forest(temp_file['tweet_text'], temp_file['cat'])\n",
    "        cat_feature_names.append(feature_names_out)\n",
    "        cat_importances.append(importances_out)\n",
    "\n",
    "    print('DONE with Training on Cat!!!')\n",
    "    \n",
    "    return content, cat_importances, cat_feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Training Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_forest(data, y):\n",
    "\n",
    "    print(\"Creating the bag of words model!\\n\")\n",
    "    # CountVectorizer\" is scikit-learn's bag of words tool, here we show more keywords \n",
    "    vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                                 tokenizer = None,    \\\n",
    "                                 preprocessor = None, \\\n",
    "                                 stop_words = None,   \\\n",
    "                                 max_features = 2000) \n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\\\n",
    "    data, y, random_state=0, test_size=.2)\n",
    "\n",
    "    # Then we use fit_transform() to fit the model / learn the vocabulary,\n",
    "    # then transform the data into feature vectors.\n",
    "    # The input should be a list of strings. .toarraty() converts to a numpy array\n",
    "    \n",
    "    train_bag = vectorizer.fit_transform(X_train).toarray()\n",
    "    test_bag = vectorizer.transform(X_test).toarray()\n",
    "\n",
    "    # You can extract the vocabulary created by CountVectorizer\n",
    "    # by running print(vectorizer.get_feature_names())\n",
    "\n",
    "\n",
    "    print(\"Training the random forest classifier!\\n\")\n",
    "    # Initialize a Random Forest classifier with 75 trees\n",
    "    forest = RandomForestClassifier(n_estimators = 50) \n",
    "\n",
    "    # Fit the forest to the training set, using the bag of words as \n",
    "    # features and the sentiment labels as the target variable\n",
    "    forest = forest.fit(train_bag, y_train)\n",
    "\n",
    "    train_predictions = forest.predict(train_bag)\n",
    "    test_predictions = forest.predict(test_bag)\n",
    "    \n",
    "    train_acc = metrics.accuracy_score(y_train, train_predictions)\n",
    "    valid_acc = metrics.accuracy_score(y_test, test_predictions)\n",
    "    print(\"The training accuracy is: \", train_acc, \"\\n\", \"The validation accuracy is: \", valid_acc)\n",
    "    \n",
    "    print('confusion matrix')\n",
    "    print(metrics.confusion_matrix(y_test,test_predictions))\n",
    "    \n",
    "    print('check classes on which we trained')\n",
    "    print(forest.classes_)\n",
    "    \n",
    "    importances = forest.feature_importances_\n",
    "    # returns relative importance of all features.\n",
    "    # they are in the order of the columns\n",
    "    print(importances)\n",
    "    len(importances)\n",
    "\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    # sort importance scores\n",
    "    \n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    # Print the feature ranking\n",
    "    print(\"Feature ranking:\")\n",
    "    top_50 = indices[:50]\n",
    "    top50_features = [vectorizer.get_feature_names()[ind] for ind in top_50]\n",
    "    print(top50_features)\n",
    "    \n",
    "    return(forest, vectorizer, feature_names, importances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
