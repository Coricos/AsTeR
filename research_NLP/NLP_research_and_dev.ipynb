{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Readings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Text summarizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on https://towardsdatascience.com/summarizing-tweets-in-a-disaster-part-ii-67db021d378d:\n",
    "- look for situational words, describing situation or casulties using SpaCy (Numerals (eg. number of casualties, important phone numbers); Entities (eg. places, dates, events, organisations, etc.))\n",
    "    - use entity-types, look for content words\n",
    "- tf-idf score (rank somthing like \"Nepal\" highly, but not \"the\") --> use Textacy\n",
    "- clean data before tokenizing: abbreviations, misspellings (NLTK has a twitter-specific tokenizer)\n",
    "- summary of words as an ILP problem\n",
    "\n",
    "check also the notebooks\n",
    "- for SpaCy: https://github.com/gabrieltseng/datascience-projects/blob/master/natural_language_processing/twitter_disasters/spaCy/3%20-%20Abstractive%20Summary.ipynb\n",
    "- for NLTK: https://github.com/gabrieltseng/datascience-projects/blob/master/natural_language_processing/twitter_disasters/NLTK/3%20-%20Abstractive%20Summary.ipynb\n",
    "\n",
    "IBM Watson research paper\n",
    "- https://arxiv.org/pdf/1602.06023.pdf\n",
    "\n",
    "Tensorflow text summarization model\n",
    "- https://github.com/tensorflow/models/tree/master/research/textsum\n",
    "\n",
    "API services\n",
    "- https://smmry.com/api\n",
    "\n",
    "Facebook AI research: A Neural Attention Model for Abstractive Sentence Summarization\n",
    "- https://arxiv.org/pdf/1509.00685.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ideas for overall approach: use occuring tweets as well (e.g. twitter set for wildfire)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Keyword Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- based on https://medium.com/analytics-vidhya/automated-keyword-extraction-from-articles-using-nlp-bfd864f41b34\n",
    "- also very interesting points on text pre-processing in here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"KeyWordExtraction_HighLevel.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Futher NLP Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- word embeddings: https://www.wikiwand.com/en/Word_embedding --> check word2vec\n",
    "- sentiment analysis: https://www.wikiwand.com/en/Sentiment_analysis\n",
    "    - for background on singular value decomposition https://www.wikiwand.com/en/Singular_value_decomposition\n",
    "- part-of-speech (POS) tagging\n",
    "- using word graphs (powerful when there are multiple sentences describing similar situations)\n",
    "- linguistic quality: compare my sample sentence to \"normal\" English sentences\n",
    "    - see also KenLM tool at https://kheafield.com/code/kenlm/\n",
    "    - and more readings to understand this challenge http://masatohagiwara.net/training-an-n-gram-language-model-and-estimating-sentence-probability.html\n",
    "    - can be compared to current \"correct\" American English https://www.english-corpora.org/coca/\n",
    "- spell checker: https://pypi.org/project/pyspellchecker/\n",
    "- regular expressions: https://docs.python.org/3/library/re.html\n",
    "- term frequency * Inverse Document Frequency: https://hackernoon.com/finding-the-most-important-sentences-using-nlp-tf-idf-3065028897a3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pre-trained language models\n",
    "- ELMo: https://arxiv.org/abs/1802.05365\n",
    "- ULMFiT: https://arxiv.org/abs/1801.06146\n",
    "- OpenAI Transformer: https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf\n",
    "- BERT: https://arxiv.org/abs/1810.04805"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP trends\n",
    "- Commonsense Interference like Event2Mind (https://arxiv.org/pdf/1805.06939.pdf) or SWAG (https://arxiv.org/abs/1808.05326)\n",
    "\n",
    "- summary of trends to be found here: http://ruder.io/10-exciting-ideas-of-2018-in-nlp/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### more research to be done into\n",
    "- general summarization\n",
    "- statistical parsing\n",
    "- knowledge extraction: are 911 calls given in a standard or re-occuring format?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of current trends in NLP\n",
    "- https://www.analyticsvidhya.com/blog/2017/10/essential-nlp-guide-data-scientists-top-10-nlp-tasks/ (includes a lot of interesting and helpful links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. DL/ ML tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- transfer learning: https://machinelearningmastery.com/transfer-learning-for-deep-learning/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Disaster datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Twitter datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://arxiv.org/abs/1605.05894\n",
    "- https://www.aaai.org/ocs/index.php/ICWSM/ICWSM11/paper/view/2834\n",
    "- https://dl.acm.org/citation.cfm?id=2914600"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Other datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://data.world/crowdflower/disasters-on-social-media\n",
    "- collection of different datasets: https://crisisnlp.qcri.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Other github links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Twitter: disaster classification, sentiment analysis, named entity recognition --> https://github.com/glrn/nlp-disaster-analysis\n",
    "- Natural Language Understanding Bot translating unstructured text into structured data --> https://github.com/Kontikilabs/alter-nlu\n",
    "- Emogram (Text Analysis for unstructured text): Acronym Resolution, Auto Corect, Key Phrase Extraction, Polarity Detection --> https://github.com/axenhammer/Emogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import bs4 as bs\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize # tokenizes sentences\n",
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "eng_stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load more data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "source data from https://crisisnlp.qcri.org/lrec2016/lrec2016.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = pd.read_csv('2013_pakistan_eq.csv', skip_blank_lines=True, encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['_unit_id', '_golden', '_unit_state', '_trusted_judgments',\n",
       "       '_last_judgment_at', 'choose_one_category',\n",
       "       'choose_one_category:confidence', 'choose_one_category_gold',\n",
       "       'tweet_id', 'tweet_text'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = file.drop(['_unit_id', '_golden', '_trusted_judgments',\n",
    "       '_last_judgment_at', 'choose_one_category:confidence', 'choose_one_category_gold',\n",
    "       'tweet_id'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['other_useful_information', 'not_related_or_irrelevant',\n",
       "       'donation_needs_or_offers_or_volunteering_services',\n",
       "       'injured_or_dead_people', 'missing_trapped_or_found_people',\n",
       "       'caution_and_advice', 'infrastructure_and_utilities_damage',\n",
       "       'sympathy_and_emotional_support',\n",
       "       'displaced_people_and_evacuations'], dtype=object)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file.choose_one_category.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reduced = file[file.choose_one_category == 'missing_trapped_or_found_people']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT @Fahdhusain: 11 kids recovered alive from under earthquake rubble in Awaran. Shukar Allah!! #earthquake\n",
      "Situation of #Balochistan: 18000+ Balochs are missing, thousands killed, no relief for #BalochistanEarthquake victims.#ReleaseAbductedBaloch\n",
      "RT @AnjumKiani: CJP have you found out where ur 'Missing Persons' are? They are attacking #Earthquake relief workers, Meds teams &amp; SoldiersÃ¢â¬Â¦\n",
      "RT @Atta_Waqas: #earthquake pk Army and Pk Govt... trying their level best for Rescue..  WHERE r all Baloch Nationalist Sardaars and BLA...?\n",
      "CJP have you found out where your 'Missing Persons' are? They are attacking #Earthquake relief workers, Med teams &amp; Soldiers. #Balochistan\n"
     ]
    }
   ],
   "source": [
    "for i in reduced.tweet_text.values:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cleaning steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RT @Atta_Waqas: #earthquake pk Army and Pk Govt... trying their level best for Rescue..  WHERE r all Baloch Nationalist Sardaars and BLA...?'"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduced.iloc[3].tweet_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove twitter specific \n",
    "#file.tweet_text = re.sub(r'http\\S+', '', file.tweet_text)\n",
    "#remove hyperlinks\n",
    "reduced = reduced.replace(to_replace =r'http\\S+', value = '', regex = True)\n",
    "#remove usernames\n",
    "reduced = reduced.replace(to_replace =r'@[A-Za-z0-9]+', value = '', regex = True) \n",
    "#remove hashtags\n",
    "#reduced = reduced.replace(to_replace =r'#[A-Za-z0-9]+', value = '', regex = True) \n",
    "# or just remove the hashtag, but leave the actual word\n",
    "reduced = reduced.replace(to_replace ='#', value = '', regex = True) \n",
    "#remove retweet\n",
    "reduced = reduced.replace(to_replace ='RT :', value = '', regex = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuation\n",
    "reduced.tweet_text = reduced.tweet_text.replace(to_replace ='[^a-zA-Z]', value = ' ', regex = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. Tokenize into words (all lower case)\n",
    "reduced.tweet_text = reduced.tweet_text.str.lower()\n",
    "reduced.tweet_text = reduced.tweet_text.str.split() \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_unit_state</th>\n",
       "      <th>choose_one_category</th>\n",
       "      <th>tweet_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>finalized</td>\n",
       "      <td>missing_trapped_or_found_people</td>\n",
       "      <td>[kids, recovered, alive, from, under, earthqua...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1082</th>\n",
       "      <td>finalized</td>\n",
       "      <td>missing_trapped_or_found_people</td>\n",
       "      <td>[situation, of, balochistan, balochs, are, mis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1126</th>\n",
       "      <td>finalized</td>\n",
       "      <td>missing_trapped_or_found_people</td>\n",
       "      <td>[cjp, have, you, found, out, where, ur, missin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1210</th>\n",
       "      <td>finalized</td>\n",
       "      <td>missing_trapped_or_found_people</td>\n",
       "      <td>[rt, waqas, earthquake, pk, army, and, pk, gov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1366</th>\n",
       "      <td>finalized</td>\n",
       "      <td>missing_trapped_or_found_people</td>\n",
       "      <td>[cjp, have, you, found, out, where, your, miss...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     _unit_state              choose_one_category  \\\n",
       "17     finalized  missing_trapped_or_found_people   \n",
       "1082   finalized  missing_trapped_or_found_people   \n",
       "1126   finalized  missing_trapped_or_found_people   \n",
       "1210   finalized  missing_trapped_or_found_people   \n",
       "1366   finalized  missing_trapped_or_found_people   \n",
       "\n",
       "                                             tweet_text  \n",
       "17    [kids, recovered, alive, from, under, earthqua...  \n",
       "1082  [situation, of, balochistan, balochs, are, mis...  \n",
       "1126  [cjp, have, you, found, out, where, ur, missin...  \n",
       "1210  [rt, waqas, earthquake, pk, army, and, pk, gov...  \n",
       "1366  [cjp, have, you, found, out, where, your, miss...  "
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5. Remove stopwords\n",
    "eng_stopwords = set(stopwords.words(\"english\"))\n",
    "review = [w for w in review if not w in eng_stopwords]\n",
    "    \n",
    "#6. Join the review to one sentence\n",
    "review = ' '.join(review+emoticons)\n",
    "# add emoticons to the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correct grammer --> slow\n",
    "correct grammer, something along these lines: https://pypi.org/project/pyspellchecker/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "liste = reduced.iloc[2].tweet_text.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "falsch {'', 'cjp', 'are?', 'soldiersã¢â\\x82¬â¦', 'workers,', 'meds', \"persons'\", '&amp;', \"'missing\"}\n",
      "a\n",
      "cup\n",
      "are\n",
      "soldiersã¢â¬â¦\n",
      "workers\n",
      "mess\n",
      "persons\n",
      "camp\n",
      "missing\n"
     ]
    }
   ],
   "source": [
    "from spellchecker import SpellChecker\n",
    "\n",
    "spell = SpellChecker()\n",
    "\n",
    "# find those words that may be misspelled\n",
    "#misspelled = spell.unknown(['something', 'is', 'hapenning', 'here'])\n",
    "misspelled = spell.unknown(liste)\n",
    "print('falsch', misspelled)\n",
    "\n",
    "for word in misspelled:\n",
    "    # Get the one `most likely` answer\n",
    "    print(spell.correction(word))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named entity recognition/ disambiguiation\n",
    "- find out name of school, city, street etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis\n",
    "- - sentiment analysis \n",
    "    - check paper at https://www.analyticsvidhya.com/blog/2017/01/sentiment-analysis-of-twitter-posts-on-chennai-floods-using-python/, where sentiment analysis was performed on Chennai flood dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output: counting expressions (like Sandy Hook School or shooting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return 'n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi sandy hook school i think there be somebody shoot in here in sandy hook school because somebody get a gun i catch a glimpse of someone theyre run down the hallway they be still run theyre still shoot sandy hook school please\n"
     ]
    }
   ],
   "source": [
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "wnl_stems = []\n",
    "for pair in token_tag:\n",
    "    res = wnl.lemmatize(pair[0],pos=get_wordnet_pos(pair[1]))\n",
    "    wnl_stems.append(res)\n",
    "\n",
    "print(' '.join(wnl_stems))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REVIEW WITHOUT STOPWORDS:\n",
      "hi sandy hook school think somebody shooting sandy hook school somebodys got gun caught glimpse someone theyre running hallway still running theyre still shooting sandy hook school please\n",
      "\n",
      "Stop words removed ['i', 'there', 'is', 'in', 'here', 'in', 'because', 'a', 'i', 'a', 'of', 'down', 'the', 'they', 'are']\n",
      "\n",
      "NUMBER OF STOPWORDS REMOVED: 15\n"
     ]
    }
   ],
   "source": [
    "tsc_wo_stopwords = [w for w in tsc_words if not w in stopwords.words(\"english\")]\n",
    "removed_stopwords = [w for w in tsc_words if w in stopwords.words(\"english\")]\n",
    "\n",
    "print('REVIEW WITHOUT STOPWORDS:')\n",
    "print(' '.join(tsc_wo_stopwords))\n",
    "print()\n",
    "print('Stop words removed', removed_stopwords)\n",
    "print()\n",
    "print('NUMBER OF STOPWORDS REMOVED:',len(removed_stopwords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### here follows a summary of what we extracted from the text (summary, keywords etc.) and how this influences the priority"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### processing: spot what is an emergency situation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### recommend steps what and how to do it --> what to employ and where to employ to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### some more links about what we can do\n",
    "- https://blog.paralleldots.com/research/artificial-intelligence-can-make-public-transportation-safer/?source=post_page---------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
